
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ COMMAND ‚îÇ                ARGS                ‚îÇ PROFILE  ‚îÇ USER  ‚îÇ VERSION ‚îÇ     START TIME      ‚îÇ      END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start   ‚îÇ --driver=docker                    ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 14:25 WIB ‚îÇ 08 Jan 26 14:26 WIB ‚îÇ
‚îÇ start   ‚îÇ --driver=docker                    ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 14:26 WIB ‚îÇ 08 Jan 26 14:27 WIB ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                     ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 14:27 WIB ‚îÇ 08 Jan 26 14:27 WIB ‚îÇ
‚îÇ tunnel  ‚îÇ                                    ‚îÇ minikube ‚îÇ root  ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 14:27 WIB ‚îÇ                     ‚îÇ
‚îÇ stop    ‚îÇ                                    ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 15:19 WIB ‚îÇ 08 Jan 26 15:20 WIB ‚îÇ
‚îÇ start   ‚îÇ --driver=docker                    ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 15:29 WIB ‚îÇ 08 Jan 26 15:29 WIB ‚îÇ
‚îÇ service ‚îÇ app-service --url                  ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 15:34 WIB ‚îÇ                     ‚îÇ
‚îÇ service ‚îÇ hello-service                      ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 15:49 WIB ‚îÇ                     ‚îÇ
‚îÇ service ‚îÇ hello-service                      ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 15:50 WIB ‚îÇ                     ‚îÇ
‚îÇ service ‚îÇ hello-service                      ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 15:50 WIB ‚îÇ                     ‚îÇ
‚îÇ start   ‚îÇ                                    ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 15:52 WIB ‚îÇ 08 Jan 26 15:53 WIB ‚îÇ
‚îÇ service ‚îÇ hello-service                      ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 15:57 WIB ‚îÇ                     ‚îÇ
‚îÇ start   ‚îÇ --driver=docker                    ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 10:19 WIB ‚îÇ 09 Jan 26 10:19 WIB ‚îÇ
‚îÇ start   ‚îÇ --driver=docker                    ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 14:06 WIB ‚îÇ 09 Jan 26 14:07 WIB ‚îÇ
‚îÇ service ‚îÇ flask-app-service --url            ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 14:09 WIB ‚îÇ                     ‚îÇ
‚îÇ service ‚îÇ flask-app-service --url            ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 14:14 WIB ‚îÇ 09 Jan 26 14:15 WIB ‚îÇ
‚îÇ service ‚îÇ flask-app-service -n staging --url ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 14:51 WIB ‚îÇ 09 Jan 26 14:52 WIB ‚îÇ
‚îÇ start   ‚îÇ                                    ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 12 Jan 26 09:40 WIB ‚îÇ 12 Jan 26 09:40 WIB ‚îÇ
‚îÇ addons  ‚îÇ enable metrics-server              ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 12 Jan 26 10:00 WIB ‚îÇ 12 Jan 26 10:00 WIB ‚îÇ
‚îÇ tunnel  ‚îÇ                                    ‚îÇ minikube ‚îÇ kyput ‚îÇ v1.37.0 ‚îÇ 12 Jan 26 10:03 WIB ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2026/01/12 09:40:12
Running on machine: Kiky-Putra
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0112 09:40:12.952142    4074 out.go:360] Setting OutFile to fd 1 ...
I0112 09:40:12.952887    4074 out.go:413] isatty.IsTerminal(1) = true
I0112 09:40:12.952890    4074 out.go:374] Setting ErrFile to fd 2...
I0112 09:40:12.952894    4074 out.go:413] isatty.IsTerminal(2) = true
I0112 09:40:12.953103    4074 root.go:338] Updating PATH: /home/kyput/.minikube/bin
W0112 09:40:12.953204    4074 root.go:314] Error reading config file at /home/kyput/.minikube/config/config.json: open /home/kyput/.minikube/config/config.json: no such file or directory
I0112 09:40:12.954124    4074 out.go:368] Setting JSON to false
I0112 09:40:12.955140    4074 start.go:130] hostinfo: {"hostname":"Kiky-Putra","uptime":587,"bootTime":1768185026,"procs":48,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.6.87.2-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"guest","hostId":"0f1ada86-c65c-464a-8e14-ae34f0e89c1f"}
I0112 09:40:12.955216    4074 start.go:140] virtualization: kvm guest
I0112 09:40:12.961788    4074 out.go:179] üòÑ  minikube v1.37.0 on Ubuntu 24.04 (kvm/amd64)
I0112 09:40:12.973512    4074 notify.go:220] Checking for updates...
I0112 09:40:12.973745    4074 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0112 09:40:12.974588    4074 driver.go:421] Setting default libvirt URI to qemu:///system
I0112 09:40:13.011471    4074 docker.go:123] docker version: linux-28.2.2:
I0112 09:40:13.011562    4074 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0112 09:40:13.041946    4074 info.go:266] docker info: {ID:edad1a02-aae9-4d81-af5d-696f6d9208fd Containers:7 ContainersRunning:0 ContainersPaused:0 ContainersStopped:7 Images:49 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:33 OomKillDisable:false NGoroutines:45 SystemTime:2026-01-12 09:40:13.028824862 +0700 WIB LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:7919038464 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Kiky-Putra Labels:[] ExperimentalBuild:false ServerVersion:28.2.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0112 09:40:13.042019    4074 docker.go:318] overlay module found
I0112 09:40:13.048190    4074 out.go:179] ‚ú®  Using the docker driver based on existing profile
I0112 09:40:13.059217    4074 start.go:304] selected driver: docker
I0112 09:40:13.059229    4074 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0112 09:40:13.059307    4074 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0112 09:40:13.059449    4074 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0112 09:40:13.090901    4074 info.go:266] docker info: {ID:edad1a02-aae9-4d81-af5d-696f6d9208fd Containers:7 ContainersRunning:0 ContainersPaused:0 ContainersStopped:7 Images:49 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:33 OomKillDisable:false NGoroutines:45 SystemTime:2026-01-12 09:40:13.078210111 +0700 WIB LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:7919038464 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Kiky-Putra Labels:[] ExperimentalBuild:false ServerVersion:28.2.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0112 09:40:13.092276    4074 cni.go:84] Creating CNI manager for ""
I0112 09:40:13.092796    4074 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0112 09:40:13.092848    4074 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0112 09:40:13.098996    4074 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0112 09:40:13.104588    4074 cache.go:123] Beginning downloading kic base image for docker with docker
I0112 09:40:13.110424    4074 out.go:179] üöú  Pulling base image v0.0.48 ...
I0112 09:40:13.116204    4074 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0112 09:40:13.116282    4074 preload.go:146] Found local preload: /home/kyput/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I0112 09:40:13.116310    4074 cache.go:58] Caching tarball of preloaded images
I0112 09:40:13.116519    4074 preload.go:172] Found /home/kyput/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0112 09:40:13.116530    4074 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I0112 09:40:13.116615    4074 profile.go:143] Saving config to /home/kyput/.minikube/profiles/minikube/config.json ...
I0112 09:40:13.116676    4074 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I0112 09:40:13.160288    4074 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon, skipping pull
I0112 09:40:13.160297    4074 cache.go:147] gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in daemon, skipping load
I0112 09:40:13.160319    4074 cache.go:232] Successfully downloaded all kic artifacts
I0112 09:40:13.160338    4074 start.go:360] acquireMachinesLock for minikube: {Name:mk93d51a0f15a69eb3b117c9b635294317e41924 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0112 09:40:13.160445    4074 start.go:364] duration metric: took 95.322¬µs to acquireMachinesLock for "minikube"
I0112 09:40:13.160460    4074 start.go:96] Skipping create...Using existing machine configuration
I0112 09:40:13.160463    4074 fix.go:54] fixHost starting: 
I0112 09:40:13.160652    4074 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0112 09:40:13.182596    4074 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0112 09:40:13.182618    4074 fix.go:138] unexpected machine state, will restart: <nil>
I0112 09:40:13.188592    4074 out.go:252] üîÑ  Restarting existing docker container for "minikube" ...
I0112 09:40:13.188732    4074 cli_runner.go:164] Run: docker start minikube
I0112 09:40:13.810797    4074 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0112 09:40:13.840474    4074 kic.go:430] container "minikube" state is running.
I0112 09:40:13.840976    4074 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0112 09:40:13.871920    4074 profile.go:143] Saving config to /home/kyput/.minikube/profiles/minikube/config.json ...
I0112 09:40:13.872279    4074 machine.go:93] provisionDockerMachine start ...
I0112 09:40:13.872376    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0112 09:40:13.902073    4074 main.go:141] libmachine: Using SSH client type: native
I0112 09:40:13.902712    4074 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0112 09:40:13.902720    4074 main.go:141] libmachine: About to run SSH command:
hostname
I0112 09:40:13.903811    4074 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:44244->127.0.0.1:32768: read: connection reset by peer
I0112 09:40:17.051987    4074 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0112 09:40:17.051999    4074 ubuntu.go:182] provisioning hostname "minikube"
I0112 09:40:17.052067    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0112 09:40:17.074614    4074 main.go:141] libmachine: Using SSH client type: native
I0112 09:40:17.074833    4074 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0112 09:40:17.074840    4074 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0112 09:40:17.247562    4074 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0112 09:40:17.247625    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0112 09:40:17.270967    4074 main.go:141] libmachine: Using SSH client type: native
I0112 09:40:17.271277    4074 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0112 09:40:17.271289    4074 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0112 09:40:17.411644    4074 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0112 09:40:17.411658    4074 ubuntu.go:188] set auth options {CertDir:/home/kyput/.minikube CaCertPath:/home/kyput/.minikube/certs/ca.pem CaPrivateKeyPath:/home/kyput/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/kyput/.minikube/machines/server.pem ServerKeyPath:/home/kyput/.minikube/machines/server-key.pem ClientKeyPath:/home/kyput/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/kyput/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/kyput/.minikube}
I0112 09:40:17.411670    4074 ubuntu.go:190] setting up certificates
I0112 09:40:17.411677    4074 provision.go:84] configureAuth start
I0112 09:40:17.411744    4074 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0112 09:40:17.433596    4074 provision.go:143] copyHostCerts
I0112 09:40:17.434394    4074 exec_runner.go:144] found /home/kyput/.minikube/ca.pem, removing ...
I0112 09:40:17.434408    4074 exec_runner.go:203] rm: /home/kyput/.minikube/ca.pem
I0112 09:40:17.434476    4074 exec_runner.go:151] cp: /home/kyput/.minikube/certs/ca.pem --> /home/kyput/.minikube/ca.pem (1074 bytes)
I0112 09:40:17.434838    4074 exec_runner.go:144] found /home/kyput/.minikube/cert.pem, removing ...
I0112 09:40:17.434846    4074 exec_runner.go:203] rm: /home/kyput/.minikube/cert.pem
I0112 09:40:17.434921    4074 exec_runner.go:151] cp: /home/kyput/.minikube/certs/cert.pem --> /home/kyput/.minikube/cert.pem (1119 bytes)
I0112 09:40:17.435283    4074 exec_runner.go:144] found /home/kyput/.minikube/key.pem, removing ...
I0112 09:40:17.435287    4074 exec_runner.go:203] rm: /home/kyput/.minikube/key.pem
I0112 09:40:17.435327    4074 exec_runner.go:151] cp: /home/kyput/.minikube/certs/key.pem --> /home/kyput/.minikube/key.pem (1675 bytes)
I0112 09:40:17.435616    4074 provision.go:117] generating server cert: /home/kyput/.minikube/machines/server.pem ca-key=/home/kyput/.minikube/certs/ca.pem private-key=/home/kyput/.minikube/certs/ca-key.pem org=kyput.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0112 09:40:17.517464    4074 provision.go:177] copyRemoteCerts
I0112 09:40:17.517541    4074 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0112 09:40:17.517592    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0112 09:40:17.542182    4074 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kyput/.minikube/machines/minikube/id_rsa Username:docker}
I0112 09:40:17.648883    4074 ssh_runner.go:362] scp /home/kyput/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0112 09:40:17.694217    4074 ssh_runner.go:362] scp /home/kyput/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0112 09:40:17.734777    4074 ssh_runner.go:362] scp /home/kyput/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0112 09:40:17.776073    4074 provision.go:87] duration metric: took 364.384176ms to configureAuth
I0112 09:40:17.776090    4074 ubuntu.go:206] setting minikube options for container-runtime
I0112 09:40:17.776256    4074 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0112 09:40:17.776313    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0112 09:40:17.800176    4074 main.go:141] libmachine: Using SSH client type: native
I0112 09:40:17.800402    4074 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0112 09:40:17.800408    4074 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0112 09:40:17.939871    4074 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0112 09:40:17.939883    4074 ubuntu.go:71] root file system type: overlay
I0112 09:40:17.939980    4074 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0112 09:40:17.940055    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0112 09:40:17.961817    4074 main.go:141] libmachine: Using SSH client type: native
I0112 09:40:17.962046    4074 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0112 09:40:17.962117    4074 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0112 09:40:18.136520    4074 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0112 09:40:18.136608    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0112 09:40:18.167390    4074 main.go:141] libmachine: Using SSH client type: native
I0112 09:40:18.167670    4074 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0112 09:40:18.167695    4074 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0112 09:40:18.336315    4074 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0112 09:40:18.336329    4074 machine.go:96] duration metric: took 4.464041076s to provisionDockerMachine
I0112 09:40:18.336343    4074 start.go:293] postStartSetup for "minikube" (driver="docker")
I0112 09:40:18.336352    4074 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0112 09:40:18.336436    4074 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0112 09:40:18.336485    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0112 09:40:18.364856    4074 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kyput/.minikube/machines/minikube/id_rsa Username:docker}
I0112 09:40:18.477514    4074 ssh_runner.go:195] Run: cat /etc/os-release
I0112 09:40:18.482380    4074 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0112 09:40:18.482392    4074 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0112 09:40:18.482397    4074 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0112 09:40:18.482401    4074 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0112 09:40:18.482409    4074 filesync.go:126] Scanning /home/kyput/.minikube/addons for local assets ...
I0112 09:40:18.482822    4074 filesync.go:126] Scanning /home/kyput/.minikube/files for local assets ...
I0112 09:40:18.483139    4074 start.go:296] duration metric: took 146.787154ms for postStartSetup
I0112 09:40:18.483220    4074 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0112 09:40:18.483257    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0112 09:40:18.504660    4074 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kyput/.minikube/machines/minikube/id_rsa Username:docker}
I0112 09:40:18.601381    4074 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0112 09:40:18.607477    4074 fix.go:56] duration metric: took 5.44700916s for fixHost
I0112 09:40:18.607492    4074 start.go:83] releasing machines lock for "minikube", held for 5.447041158s
I0112 09:40:18.607576    4074 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0112 09:40:18.630182    4074 ssh_runner.go:195] Run: cat /version.json
I0112 09:40:18.630238    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0112 09:40:18.630289    4074 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0112 09:40:18.630335    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0112 09:40:18.655078    4074 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kyput/.minikube/machines/minikube/id_rsa Username:docker}
I0112 09:40:18.656164    4074 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kyput/.minikube/machines/minikube/id_rsa Username:docker}
I0112 09:40:19.428300    4074 ssh_runner.go:195] Run: systemctl --version
I0112 09:40:19.437462    4074 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0112 09:40:19.443823    4074 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0112 09:40:19.471906    4074 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0112 09:40:19.471967    4074 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0112 09:40:19.485462    4074 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0112 09:40:19.485485    4074 start.go:495] detecting cgroup driver to use...
I0112 09:40:19.485525    4074 detect.go:190] detected "systemd" cgroup driver on host os
I0112 09:40:19.486046    4074 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0112 09:40:19.509654    4074 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I0112 09:40:19.525040    4074 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0112 09:40:19.539599    4074 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0112 09:40:19.539676    4074 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0112 09:40:19.554344    4074 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0112 09:40:19.568983    4074 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0112 09:40:19.583580    4074 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0112 09:40:19.597841    4074 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0112 09:40:19.611890    4074 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0112 09:40:19.626894    4074 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0112 09:40:19.642075    4074 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0112 09:40:19.657013    4074 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0112 09:40:19.669879    4074 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0112 09:40:19.683369    4074 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0112 09:40:19.778972    4074 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0112 09:40:19.896475    4074 start.go:495] detecting cgroup driver to use...
I0112 09:40:19.896516    4074 detect.go:190] detected "systemd" cgroup driver on host os
I0112 09:40:19.896596    4074 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0112 09:40:19.915009    4074 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0112 09:40:19.931159    4074 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0112 09:40:19.978870    4074 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0112 09:40:19.994497    4074 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0112 09:40:20.013276    4074 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0112 09:40:20.041459    4074 ssh_runner.go:195] Run: which cri-dockerd
I0112 09:40:20.047386    4074 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0112 09:40:20.065177    4074 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I0112 09:40:20.097420    4074 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0112 09:40:20.207164    4074 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0112 09:40:20.295846    4074 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I0112 09:40:20.295923    4074 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0112 09:40:20.321178    4074 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0112 09:40:20.336885    4074 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0112 09:40:20.435038    4074 ssh_runner.go:195] Run: sudo systemctl restart docker
I0112 09:40:22.195191    4074 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.76013218s)
I0112 09:40:22.195249    4074 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I0112 09:40:22.210759    4074 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0112 09:40:22.225397    4074 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0112 09:40:22.242525    4074 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0112 09:40:22.258198    4074 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0112 09:40:22.350732    4074 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0112 09:40:22.438566    4074 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0112 09:40:22.535766    4074 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0112 09:40:22.589919    4074 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0112 09:40:22.609103    4074 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0112 09:40:22.703032    4074 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0112 09:40:23.028261    4074 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0112 09:40:23.044913    4074 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0112 09:40:23.044999    4074 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0112 09:40:23.050269    4074 start.go:563] Will wait 60s for crictl version
I0112 09:40:23.050397    4074 ssh_runner.go:195] Run: which crictl
I0112 09:40:23.054936    4074 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0112 09:40:23.187702    4074 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I0112 09:40:23.187810    4074 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0112 09:40:23.310618    4074 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0112 09:40:23.353106    4074 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I0112 09:40:23.353299    4074 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0112 09:40:23.376261    4074 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0112 09:40:23.381815    4074 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0112 09:40:23.397896    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0112 09:40:23.420104    4074 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0112 09:40:23.420202    4074 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0112 09:40:23.420266    4074 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0112 09:40:23.449857    4074 docker.go:691] Got preloaded images: -- stdout --
kikyputraa/sast-scan-test:latest
kikyputraa/sast-scan-test:<none>
kikyputraa/sast-scan-test:<none>
kikyputraa/hello-devops:latest
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0112 09:40:23.449868    4074 docker.go:621] Images already preloaded, skipping extraction
I0112 09:40:23.449939    4074 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0112 09:40:23.479487    4074 docker.go:691] Got preloaded images: -- stdout --
kikyputraa/sast-scan-test:latest
kikyputraa/sast-scan-test:<none>
kikyputraa/sast-scan-test:<none>
kikyputraa/hello-devops:latest
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0112 09:40:23.479499    4074 cache_images.go:85] Images are preloaded, skipping loading
I0112 09:40:23.479517    4074 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I0112 09:40:23.479652    4074 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0112 09:40:23.479725    4074 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0112 09:40:23.791582    4074 cni.go:84] Creating CNI manager for ""
I0112 09:40:23.791604    4074 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0112 09:40:23.791657    4074 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0112 09:40:23.791674    4074 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0112 09:40:23.791797    4074 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0112 09:40:23.791896    4074 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I0112 09:40:23.809240    4074 binaries.go:44] Found k8s binaries, skipping transfer
I0112 09:40:23.809323    4074 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0112 09:40:23.825499    4074 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0112 09:40:23.857644    4074 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0112 09:40:23.884708    4074 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I0112 09:40:23.909936    4074 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0112 09:40:23.914678    4074 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0112 09:40:23.932357    4074 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0112 09:40:24.008124    4074 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0112 09:40:24.049244    4074 certs.go:68] Setting up /home/kyput/.minikube/profiles/minikube for IP: 192.168.49.2
I0112 09:40:24.049254    4074 certs.go:194] generating shared ca certs ...
I0112 09:40:24.049266    4074 certs.go:226] acquiring lock for ca certs: {Name:mk9feb36495a0534c58602e0945836c021dad78a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0112 09:40:24.049552    4074 certs.go:235] skipping valid "minikubeCA" ca cert: /home/kyput/.minikube/ca.key
I0112 09:40:24.049884    4074 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/kyput/.minikube/proxy-client-ca.key
I0112 09:40:24.049894    4074 certs.go:256] generating profile certs ...
I0112 09:40:24.049981    4074 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/kyput/.minikube/profiles/minikube/client.key
I0112 09:40:24.050376    4074 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/kyput/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0112 09:40:24.050858    4074 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/kyput/.minikube/profiles/minikube/proxy-client.key
I0112 09:40:24.051033    4074 certs.go:484] found cert: /home/kyput/.minikube/certs/ca-key.pem (1679 bytes)
I0112 09:40:24.051061    4074 certs.go:484] found cert: /home/kyput/.minikube/certs/ca.pem (1074 bytes)
I0112 09:40:24.051083    4074 certs.go:484] found cert: /home/kyput/.minikube/certs/cert.pem (1119 bytes)
I0112 09:40:24.051108    4074 certs.go:484] found cert: /home/kyput/.minikube/certs/key.pem (1675 bytes)
I0112 09:40:24.051643    4074 ssh_runner.go:362] scp /home/kyput/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0112 09:40:24.088357    4074 ssh_runner.go:362] scp /home/kyput/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0112 09:40:24.125514    4074 ssh_runner.go:362] scp /home/kyput/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0112 09:40:24.162879    4074 ssh_runner.go:362] scp /home/kyput/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0112 09:40:24.199769    4074 ssh_runner.go:362] scp /home/kyput/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0112 09:40:24.236813    4074 ssh_runner.go:362] scp /home/kyput/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0112 09:40:24.276793    4074 ssh_runner.go:362] scp /home/kyput/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0112 09:40:24.315114    4074 ssh_runner.go:362] scp /home/kyput/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0112 09:40:24.362201    4074 ssh_runner.go:362] scp /home/kyput/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0112 09:40:24.412298    4074 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0112 09:40:24.447849    4074 ssh_runner.go:195] Run: openssl version
I0112 09:40:24.461591    4074 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0112 09:40:24.484080    4074 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0112 09:40:24.491024    4074 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jan  8 07:26 /usr/share/ca-certificates/minikubeCA.pem
I0112 09:40:24.491091    4074 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0112 09:40:24.501374    4074 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0112 09:40:24.518380    4074 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0112 09:40:24.525148    4074 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0112 09:40:24.535946    4074 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0112 09:40:24.547514    4074 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0112 09:40:24.559231    4074 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0112 09:40:24.570751    4074 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0112 09:40:24.581776    4074 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0112 09:40:24.592965    4074 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0112 09:40:24.593119    4074 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0112 09:40:24.630812    4074 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0112 09:40:24.648525    4074 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0112 09:40:24.648536    4074 kubeadm.go:589] restartPrimaryControlPlane start ...
I0112 09:40:24.648599    4074 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0112 09:40:24.670004    4074 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0112 09:40:24.670089    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0112 09:40:24.702350    4074 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:32771"
I0112 09:40:24.781236    4074 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0112 09:40:24.796412    4074 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I0112 09:40:24.796435    4074 kubeadm.go:593] duration metric: took 147.895115ms to restartPrimaryControlPlane
I0112 09:40:24.796442    4074 kubeadm.go:394] duration metric: took 203.488577ms to StartCluster
I0112 09:40:24.796454    4074 settings.go:142] acquiring lock: {Name:mk94f7e2b471345d4fa56eca053c11788a506194 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0112 09:40:24.796567    4074 settings.go:150] Updating kubeconfig:  /home/kyput/.kube/config
I0112 09:40:24.800289    4074 lock.go:35] WriteFile acquiring /home/kyput/.kube/config: {Name:mk4bfa07037908b2e77e5b32c8bd3138c27991da Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0112 09:40:24.800657    4074 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0112 09:40:24.800811    4074 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0112 09:40:24.802822    4074 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:true ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0112 09:40:24.802901    4074 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0112 09:40:24.802917    4074 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0112 09:40:24.802920    4074 addons.go:247] addon storage-provisioner should already be in state true
I0112 09:40:24.802924    4074 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0112 09:40:24.802933    4074 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0112 09:40:24.802940    4074 host.go:66] Checking if "minikube" exists ...
I0112 09:40:24.802972    4074 addons.go:69] Setting ingress=true in profile "minikube"
I0112 09:40:24.802986    4074 addons.go:238] Setting addon ingress=true in "minikube"
W0112 09:40:24.802991    4074 addons.go:247] addon ingress should already be in state true
I0112 09:40:24.803012    4074 host.go:66] Checking if "minikube" exists ...
I0112 09:40:24.803169    4074 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0112 09:40:24.803224    4074 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0112 09:40:24.803285    4074 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0112 09:40:24.811421    4074 out.go:179] üîé  Verifying Kubernetes components...
I0112 09:40:24.825858    4074 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0112 09:40:24.835654    4074 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0112 09:40:24.835664    4074 addons.go:247] addon default-storageclass should already be in state true
I0112 09:40:24.835686    4074 host.go:66] Checking if "minikube" exists ...
I0112 09:40:24.836006    4074 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0112 09:40:24.844884    4074 out.go:179]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2
I0112 09:40:24.844897    4074 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0112 09:40:24.859693    4074 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0112 09:40:24.859708    4074 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0112 09:40:24.859810    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0112 09:40:24.865335    4074 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0112 09:40:24.865354    4074 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0112 09:40:24.865492    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0112 09:40:24.866402    4074 out.go:179]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2
I0112 09:40:24.880175    4074 out.go:179]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/controller:v1.13.2
I0112 09:40:24.901029    4074 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kyput/.minikube/machines/minikube/id_rsa Username:docker}
I0112 09:40:24.907309    4074 addons.go:435] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0112 09:40:24.907329    4074 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (16078 bytes)
I0112 09:40:24.907441    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0112 09:40:24.910191    4074 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kyput/.minikube/machines/minikube/id_rsa Username:docker}
I0112 09:40:24.945511    4074 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kyput/.minikube/machines/minikube/id_rsa Username:docker}
I0112 09:40:25.042330    4074 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0112 09:40:25.045766    4074 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0112 09:40:25.059238    4074 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0112 09:40:25.068062    4074 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0112 09:40:25.090750    4074 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0112 09:40:25.106850    4074 api_server.go:52] waiting for apiserver process to appear ...
I0112 09:40:25.106928    4074 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0112 09:40:25.521722    4074 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0112 09:40:25.521774    4074 retry.go:31] will retry after 339.796052ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0112 09:40:25.524960    4074 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0112 09:40:25.524978    4074 retry.go:31] will retry after 144.15715ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0112 09:40:25.525143    4074 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0112 09:40:25.525152    4074 retry.go:31] will retry after 141.279342ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0112 09:40:25.607608    4074 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0112 09:40:25.666962    4074 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
I0112 09:40:25.669394    4074 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0112 09:40:25.763868    4074 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0112 09:40:25.763897    4074 retry.go:31] will retry after 535.778159ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0112 09:40:25.763963    4074 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0112 09:40:25.763978    4074 retry.go:31] will retry after 225.068358ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0112 09:40:25.862434    4074 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0112 09:40:25.961294    4074 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0112 09:40:25.961319    4074 retry.go:31] will retry after 217.479766ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0112 09:40:25.990240    4074 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0112 09:40:26.087908    4074 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0112 09:40:26.087927    4074 retry.go:31] will retry after 771.295493ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0112 09:40:26.107275    4074 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0112 09:40:26.128027    4074 api_server.go:72] duration metric: took 1.327346566s to wait for apiserver process to appear ...
I0112 09:40:26.128042    4074 api_server.go:88] waiting for apiserver healthz status ...
I0112 09:40:26.128058    4074 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0112 09:40:26.180122    4074 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0112 09:40:26.300920    4074 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
I0112 09:40:26.859905    4074 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0112 09:40:27.539320    4074 api_server.go:279] https://127.0.0.1:32771/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0112 09:40:27.539339    4074 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0112 09:40:27.539353    4074 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0112 09:40:27.585167    4074 api_server.go:279] https://127.0.0.1:32771/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0112 09:40:27.585208    4074 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0112 09:40:27.629426    4074 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0112 09:40:27.641673    4074 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0112 09:40:27.641711    4074 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0112 09:40:28.087208    4074 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.907057966s)
I0112 09:40:28.128589    4074 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0112 09:40:28.133029    4074 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0112 09:40:28.133043    4074 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0112 09:40:28.628604    4074 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0112 09:40:28.634257    4074 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0112 09:40:28.634280    4074 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0112 09:40:28.815652    4074 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: (2.514691763s)
I0112 09:40:28.815693    4074 addons.go:479] Verifying addon ingress=true in "minikube"
I0112 09:40:28.815778    4074 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.955849423s)
I0112 09:40:28.840132    4074 out.go:179] üîé  Verifying ingress addon...
I0112 09:40:28.868143    4074 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0112 09:40:28.899206    4074 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0112 09:40:28.899233    4074 kapi.go:107] duration metric: took 31.095603ms to wait for app.kubernetes.io/name=ingress-nginx ...
I0112 09:40:28.918991    4074 out.go:179] üåü  Enabled addons: storage-provisioner, default-storageclass, ingress
I0112 09:40:28.936418    4074 addons.go:514] duration metric: took 4.13559265s for enable addons: enabled=[storage-provisioner default-storageclass ingress]
I0112 09:40:29.128961    4074 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0112 09:40:29.134857    4074 api_server.go:279] https://127.0.0.1:32771/healthz returned 200:
ok
I0112 09:40:29.136500    4074 api_server.go:141] control plane version: v1.34.0
I0112 09:40:29.136524    4074 api_server.go:131] duration metric: took 3.008475318s to wait for apiserver health ...
I0112 09:40:29.136535    4074 system_pods.go:43] waiting for kube-system pods to appear ...
I0112 09:40:29.141978    4074 system_pods.go:59] 7 kube-system pods found
I0112 09:40:29.142024    4074 system_pods.go:61] "coredns-66bc5c9577-v8fzs" [0d3a1025-1484-4a46-a506-f74cc64b2300] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0112 09:40:29.142035    4074 system_pods.go:61] "etcd-minikube" [68681e1f-0685-4f69-b211-d4659b258be2] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0112 09:40:29.142045    4074 system_pods.go:61] "kube-apiserver-minikube" [aa78e641-c13b-4d6f-99ae-d02166698cfb] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0112 09:40:29.142053    4074 system_pods.go:61] "kube-controller-manager-minikube" [aaa8e9e4-da37-4f65-86e0-a93135c883bc] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0112 09:40:29.142060    4074 system_pods.go:61] "kube-proxy-bhhjb" [53532ed1-20b6-4337-a8cb-e159bc74ef45] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0112 09:40:29.142066    4074 system_pods.go:61] "kube-scheduler-minikube" [db6f4886-9068-4d48-974b-010e635ffb55] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0112 09:40:29.142072    4074 system_pods.go:61] "storage-provisioner" [d2943d29-400c-4a49-a8a5-1774ca794919] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0112 09:40:29.142081    4074 system_pods.go:74] duration metric: took 5.538995ms to wait for pod list to return data ...
I0112 09:40:29.142096    4074 kubeadm.go:578] duration metric: took 4.341417396s to wait for: map[apiserver:true system_pods:true]
I0112 09:40:29.142111    4074 node_conditions.go:102] verifying NodePressure condition ...
I0112 09:40:29.147712    4074 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0112 09:40:29.147819    4074 node_conditions.go:123] node cpu capacity is 16
I0112 09:40:29.147840    4074 node_conditions.go:105] duration metric: took 5.722514ms to run NodePressure ...
I0112 09:40:29.147861    4074 start.go:241] waiting for startup goroutines ...
I0112 09:40:29.147914    4074 start.go:246] waiting for cluster config update ...
I0112 09:40:29.147933    4074 start.go:255] writing updated cluster config ...
I0112 09:40:29.148750    4074 ssh_runner.go:195] Run: rm -f paused
I0112 09:40:29.389477    4074 start.go:617] kubectl: 1.35.0, cluster: 1.34.0 (minor skew: 1)
I0112 09:40:29.398160    4074 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jan 12 02:40:25 minikube cri-dockerd[1204]: time="2026-01-12T02:40:25Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-v8fzs_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3a3d55d3cb2b0718e4828b113e93ca20e34d4f617588a1a5ae3884f688d855ab\""
Jan 12 02:40:25 minikube cri-dockerd[1204]: time="2026-01-12T02:40:25Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-devops-677bf497c9-9ccpl_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8410a558978ef1b38b49336b71fb7178d80a6bc640139db0072f66ebfbf0c85a\""
Jan 12 02:40:27 minikube cri-dockerd[1204]: time="2026-01-12T02:40:27Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jan 12 02:40:29 minikube cri-dockerd[1204]: time="2026-01-12T02:40:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/58f8a41643a3f943200ffebd9316459cbabc077c0ccd20a93974cf386512cded/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:40:29 minikube cri-dockerd[1204]: time="2026-01-12T02:40:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0ee59920aac7f03cce4d0cc7d8581c3c3e2ff8c560775e4ba7314a3196eb3a12/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:40:29 minikube cri-dockerd[1204]: time="2026-01-12T02:40:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4384c3a23fb2dc04c04624264fb36a0d85450142a8c6171778c9fe20f586e02f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:40:29 minikube cri-dockerd[1204]: time="2026-01-12T02:40:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b1c2fe2dd3902fa1aee79db6630e81d9f078b01ebf05a3a770bb11cf35725ec0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:40:29 minikube cri-dockerd[1204]: time="2026-01-12T02:40:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fc04db9cbdc94bdd3221f7813cb163719d3a8c5b13baf8dc295f27e03a0bbaf2/resolv.conf as [nameserver 10.96.0.10 search staging.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:40:29 minikube cri-dockerd[1204]: time="2026-01-12T02:40:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eed9e799c050b37dc261b77f7bd787a4c13a199fec8b6c082bb9d5d2adf90fd0/resolv.conf as [nameserver 10.96.0.10 search staging.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:40:29 minikube cri-dockerd[1204]: time="2026-01-12T02:40:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dcd5294789fba4b4b10ea039daf24eff3f3456e5cf5412a702ddca8deea6e1ce/resolv.conf as [nameserver 192.168.49.1 search lan options ndots:0]"
Jan 12 02:40:29 minikube cri-dockerd[1204]: time="2026-01-12T02:40:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b8b539e65daffb5d45192e10947089c26222fcf28ce12a9201a9e30e7f6914ca/resolv.conf as [nameserver 192.168.49.1 search lan options ndots:0]"
Jan 12 02:40:29 minikube cri-dockerd[1204]: time="2026-01-12T02:40:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c8b89ce104bab09187d79d6dece33a97ff92dd1811c35d1acbce39a1d1b6ad45/resolv.conf as [nameserver 192.168.49.1 search lan options ndots:0]"
Jan 12 02:40:43 minikube cri-dockerd[1204]: time="2026-01-12T02:40:43Z" level=info msg="Stop pulling image kikyputraa/sast-scan-test:latest: Status: Downloaded newer image for kikyputraa/sast-scan-test:latest"
Jan 12 02:40:49 minikube cri-dockerd[1204]: time="2026-01-12T02:40:49Z" level=info msg="Stop pulling image kikyputraa/hello-devops:latest: Status: Image is up to date for kikyputraa/hello-devops:latest"
Jan 12 02:40:54 minikube cri-dockerd[1204]: time="2026-01-12T02:40:54Z" level=info msg="Stop pulling image kikyputraa/sast-scan-test:latest: Status: Image is up to date for kikyputraa/sast-scan-test:latest"
Jan 12 02:40:57 minikube dockerd[851]: time="2026-01-12T02:40:57.317408307Z" level=info msg="ignoring event" container=eee775c0d4ffe43c23f88b06f4553788e353310f0b367a76d2b85d0efc428ce6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 12 02:40:58 minikube cri-dockerd[1204]: time="2026-01-12T02:40:58Z" level=info msg="Stop pulling image kikyputraa/sast-scan-test:latest: Status: Image is up to date for kikyputraa/sast-scan-test:latest"
Jan 12 02:41:03 minikube cri-dockerd[1204]: time="2026-01-12T02:41:03Z" level=info msg="Stop pulling image kikyputraa/sast-scan-test:latest: Status: Image is up to date for kikyputraa/sast-scan-test:latest"
Jan 12 02:41:31 minikube dockerd[851]: time="2026-01-12T02:41:31.521839057Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=3050e302e7a048c33598dd460e6cd8387bb77ba7b5d3f98138426d838eba3571
Jan 12 02:41:31 minikube dockerd[851]: time="2026-01-12T02:41:31.560280788Z" level=info msg="ignoring event" container=3050e302e7a048c33598dd460e6cd8387bb77ba7b5d3f98138426d838eba3571 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 12 02:41:31 minikube dockerd[851]: time="2026-01-12T02:41:31.891012418Z" level=info msg="ignoring event" container=eed9e799c050b37dc261b77f7bd787a4c13a199fec8b6c082bb9d5d2adf90fd0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 12 02:41:32 minikube dockerd[851]: time="2026-01-12T02:41:32.228573673Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=1f9a79b519356f9a9ab1afe587c8e0f4d911fd579a7053fd72cf3d269834442f
Jan 12 02:41:32 minikube dockerd[851]: time="2026-01-12T02:41:32.268494732Z" level=info msg="ignoring event" container=1f9a79b519356f9a9ab1afe587c8e0f4d911fd579a7053fd72cf3d269834442f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 12 02:41:32 minikube cri-dockerd[1204]: time="2026-01-12T02:41:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"flask-app-deployment-658844656-p987n_staging\": unexpected command output nsenter: cannot open /proc/2759/ns/net: No such file or directory\n with error: exit status 1"
Jan 12 02:41:32 minikube dockerd[851]: time="2026-01-12T02:41:32.575516318Z" level=info msg="ignoring event" container=fc04db9cbdc94bdd3221f7813cb163719d3a8c5b13baf8dc295f27e03a0bbaf2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 12 02:42:11 minikube cri-dockerd[1204]: time="2026-01-12T02:42:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/408a346cab5e53f1e3ed67cffa6f116bd30ed4d4e07296677891e23759a53693/resolv.conf as [nameserver 10.96.0.10 search staging.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:42:11 minikube cri-dockerd[1204]: time="2026-01-12T02:42:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/78e8295cf0ddd74b405c332aa194a73f27bf0d58ad27508acd6164d79b283725/resolv.conf as [nameserver 10.96.0.10 search staging.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:42:18 minikube cri-dockerd[1204]: time="2026-01-12T02:42:18Z" level=info msg="Stop pulling image kikyputraa/sast-scan-test:latest: Status: Image is up to date for kikyputraa/sast-scan-test:latest"
Jan 12 02:42:23 minikube cri-dockerd[1204]: time="2026-01-12T02:42:23Z" level=info msg="Stop pulling image kikyputraa/sast-scan-test:latest: Status: Image is up to date for kikyputraa/sast-scan-test:latest"
Jan 12 02:44:24 minikube cri-dockerd[1204]: time="2026-01-12T02:44:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/41f623cb47dc0b146d602587935ef86ac099cf33a03d4148c585e9e0ccc3b0e9/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:44:30 minikube cri-dockerd[1204]: time="2026-01-12T02:44:30Z" level=info msg="Stop pulling image ghcr.io/jkroepke/kube-webhook-certgen:1.7.4: Status: Downloaded newer image for ghcr.io/jkroepke/kube-webhook-certgen:1.7.4"
Jan 12 02:44:31 minikube dockerd[851]: time="2026-01-12T02:44:31.066755878Z" level=info msg="ignoring event" container=7ac551edee0f38eea0f40dc0dc84620c12e47dca0ab310dd65aae521805394f1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 12 02:44:32 minikube dockerd[851]: time="2026-01-12T02:44:32.947020273Z" level=info msg="ignoring event" container=41f623cb47dc0b146d602587935ef86ac099cf33a03d4148c585e9e0ccc3b0e9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 12 02:44:35 minikube cri-dockerd[1204]: time="2026-01-12T02:44:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d8b70cfe17ebd7d751cf6a5459d711db0ced08690d9d6d1fef1bd86fdfa55449/resolv.conf as [nameserver 192.168.49.1 search lan options ndots:0]"
Jan 12 02:44:35 minikube cri-dockerd[1204]: time="2026-01-12T02:44:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0a612a52dcbd4174b34ba422ad2afd75febcbcd4df87585cbac38633cd007d99/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:44:35 minikube cri-dockerd[1204]: time="2026-01-12T02:44:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b912a0e16d460a97efcd1ba76ac0c6ace2ac7d6982ec20ba56660d88bd41078d/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:44:35 minikube cri-dockerd[1204]: time="2026-01-12T02:44:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cd47db08937d6bbbfb44de2aacac61cabb35ba2b43c9ae298847d3b7a4f2d27a/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:44:37 minikube cri-dockerd[1204]: time="2026-01-12T02:44:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a88dd80ab991edef438df438f14f58aa20f87ba93cda72fb21113469c3bbf93b/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:44:37 minikube dockerd[851]: time="2026-01-12T02:44:37.556005343Z" level=info msg="ignoring event" container=68ac13d2caa3987652d42e437fd29265cd00766638021c8da869577f2253445d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 12 02:44:39 minikube dockerd[851]: time="2026-01-12T02:44:39.149195547Z" level=info msg="ignoring event" container=a88dd80ab991edef438df438f14f58aa20f87ba93cda72fb21113469c3bbf93b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 12 02:44:47 minikube cri-dockerd[1204]: time="2026-01-12T02:44:47Z" level=info msg="Stop pulling image quay.io/prometheus/node-exporter:v1.10.2: Status: Downloaded newer image for quay.io/prometheus/node-exporter:v1.10.2"
Jan 12 02:44:56 minikube cri-dockerd[1204]: time="2026-01-12T02:44:56Z" level=info msg="Stop pulling image registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0: Status: Downloaded newer image for registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0"
Jan 12 02:45:05 minikube cri-dockerd[1204]: time="2026-01-12T02:45:05Z" level=info msg="Stop pulling image quay.io/prometheus-operator/prometheus-operator:v0.87.1: Status: Downloaded newer image for quay.io/prometheus-operator/prometheus-operator:v0.87.1"
Jan 12 02:45:06 minikube cri-dockerd[1204]: time="2026-01-12T02:45:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/789351a0c571b87d756f67764fc75f8a31d82df27303d30c5c14bbc9635f8c8d/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:45:06 minikube cri-dockerd[1204]: time="2026-01-12T02:45:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/be90c99007f607e8074326390a3d60afe297db5480ecad6d018a7fa5eae38e08/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 02:45:16 minikube cri-dockerd[1204]: time="2026-01-12T02:45:16Z" level=info msg="Stop pulling image quay.io/kiwigrid/k8s-sidecar:2.2.1: Status: Downloaded newer image for quay.io/kiwigrid/k8s-sidecar:2.2.1"
Jan 12 02:45:23 minikube cri-dockerd[1204]: time="2026-01-12T02:45:23Z" level=info msg="Stop pulling image quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1: Status: Downloaded newer image for quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1"
Jan 12 02:45:24 minikube dockerd[851]: time="2026-01-12T02:45:24.327959810Z" level=info msg="ignoring event" container=693c31716b0d9affcdea2d0655a58c807d0dc66aa30c22a81f4fa0a6194a072b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 12 02:45:27 minikube cri-dockerd[1204]: time="2026-01-12T02:45:27Z" level=info msg="Stop pulling image quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1: Status: Image is up to date for quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1"
Jan 12 02:45:27 minikube dockerd[851]: time="2026-01-12T02:45:27.651665631Z" level=info msg="ignoring event" container=99a20145844d4e5e7c37d0d610c357b23f7bc1b9a8f5bdcca66da8c897f1be32 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 12 02:45:43 minikube cri-dockerd[1204]: time="2026-01-12T02:45:43Z" level=info msg="Pulling image docker.io/grafana/grafana:12.3.1: 835a6d0e4902: Pull complete "
Jan 12 02:45:53 minikube cri-dockerd[1204]: time="2026-01-12T02:45:53Z" level=info msg="Pulling image docker.io/grafana/grafana:12.3.1: a7d3fdc2e5af: Extracting [===>                                               ]  5.014MB/83.15MB"
Jan 12 02:45:59 minikube cri-dockerd[1204]: time="2026-01-12T02:45:59Z" level=info msg="Stop pulling image docker.io/grafana/grafana:12.3.1: Status: Downloaded newer image for grafana/grafana:12.3.1"
Jan 12 02:46:15 minikube cri-dockerd[1204]: time="2026-01-12T02:46:15Z" level=info msg="Pulling image quay.io/prometheus/alertmanager:v0.30.0: 315eb988c8ec: Downloading [==>                                                ]  878.6kB/21.23MB"
Jan 12 02:46:18 minikube cri-dockerd[1204]: time="2026-01-12T02:46:18Z" level=info msg="Stop pulling image quay.io/prometheus/alertmanager:v0.30.0: Status: Downloaded newer image for quay.io/prometheus/alertmanager:v0.30.0"
Jan 12 02:46:34 minikube cri-dockerd[1204]: time="2026-01-12T02:46:34Z" level=info msg="Pulling image quay.io/prometheus/prometheus:v3.9.1: d956c9c5fe9e: Extracting [===================>                               ]  24.51MB/63.56MB"
Jan 12 02:46:35 minikube cri-dockerd[1204]: time="2026-01-12T02:46:35Z" level=info msg="Stop pulling image quay.io/prometheus/prometheus:v3.9.1: Status: Downloaded newer image for quay.io/prometheus/prometheus:v3.9.1"
Jan 12 03:00:50 minikube cri-dockerd[1204]: time="2026-01-12T03:00:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e1098e6776e7ae55ecd2211aae8f3caed911a0c5625331aaee8eaeb5b900506d/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Jan 12 03:00:51 minikube dockerd[851]: time="2026-01-12T03:00:51.358864379Z" level=warning msg="reference for unknown type: " digest="sha256:89258156d0e9af60403eafd44da9676fd66f600c7934d468ccc17e42b199aee2" remote="registry.k8s.io/metrics-server/metrics-server@sha256:89258156d0e9af60403eafd44da9676fd66f600c7934d468ccc17e42b199aee2"
Jan 12 03:01:00 minikube cri-dockerd[1204]: time="2026-01-12T03:01:00Z" level=info msg="Stop pulling image registry.k8s.io/metrics-server/metrics-server:v0.8.0@sha256:89258156d0e9af60403eafd44da9676fd66f600c7934d468ccc17e42b199aee2: Status: Downloaded newer image for registry.k8s.io/metrics-server/metrics-server@sha256:89258156d0e9af60403eafd44da9676fd66f600c7934d468ccc17e42b199aee2"


==> container status <==
CONTAINER           IMAGE                                                                                                                            CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
8a0ab42a09fd6       registry.k8s.io/metrics-server/metrics-server@sha256:89258156d0e9af60403eafd44da9676fd66f600c7934d468ccc17e42b199aee2            8 minutes ago       Running             metrics-server            0                   e1098e6776e7a       metrics-server-85b7d694d7-7pqsn
6e603201ddb66       ac4d7c7774890                                                                                                                    22 minutes ago      Running             config-reloader           0                   be90c99007f60       prometheus-obs-stack-kube-prometheus-prometheus-0
c2db0ae398421       quay.io/prometheus/prometheus@sha256:1f0f50f06acaceb0f5670d2c8a658a599affe7b0d8e78b898c1035653849a702                            22 minutes ago      Running             prometheus                0                   be90c99007f60       prometheus-obs-stack-kube-prometheus-prometheus-0
cf4675e7ce10f       ac4d7c7774890                                                                                                                    23 minutes ago      Running             config-reloader           0                   789351a0c571b       alertmanager-obs-stack-kube-prometheus-alertmanager-0
856d57db35805       quay.io/prometheus/alertmanager@sha256:abb750ac7b63116761c16dd481ae92496fbe04721686c0920f0fa4d0728cd4a6                          23 minutes ago      Running             alertmanager              0                   789351a0c571b       alertmanager-obs-stack-kube-prometheus-alertmanager-0
ed73deb52ff19       grafana/grafana@sha256:2175aaa91c96733d86d31cf270d5310b278654b03f5718c59de12a865380a31f                                          23 minutes ago      Running             grafana                   0                   cd47db08937d6       obs-stack-grafana-74958449f-szw2h
99a20145844d4       quay.io/prometheus-operator/prometheus-config-reloader@sha256:257ce3543e8f45c059dab488bcf897d8cc598cabd78afba12f41d6dbe36e22d2   24 minutes ago      Exited              init-config-reloader      0                   be90c99007f60       prometheus-obs-stack-kube-prometheus-prometheus-0
693c31716b0d9       quay.io/prometheus-operator/prometheus-config-reloader@sha256:257ce3543e8f45c059dab488bcf897d8cc598cabd78afba12f41d6dbe36e22d2   24 minutes ago      Exited              init-config-reloader      0                   789351a0c571b       alertmanager-obs-stack-kube-prometheus-alertmanager-0
cccbb64b090de       28282f62d5295                                                                                                                    24 minutes ago      Running             grafana-sc-datasources    0                   cd47db08937d6       obs-stack-grafana-74958449f-szw2h
c6aecd4fee9ad       quay.io/kiwigrid/k8s-sidecar@sha256:a36a5946ea215ca6435a2a7e155a17cc360ac35664ad33fa8f640a1d6786100e                             24 minutes ago      Running             grafana-sc-dashboard      0                   cd47db08937d6       obs-stack-grafana-74958449f-szw2h
f2d3a4214263d       quay.io/prometheus-operator/prometheus-operator@sha256:6dbbbbeca6d7b94aa30723bfaa55262e41b9f8c15304c484611c696503840aac          24 minutes ago      Running             kube-prometheus-stack     0                   b912a0e16d460       obs-stack-kube-prometheus-operator-64587f75b5-8xwm8
42a23284f7639       registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:2bbc915567334b13632bf62c0a97084aff72a36e13c4dabd5f2f11c898c5bacd    24 minutes ago      Running             kube-state-metrics        0                   0a612a52dcbd4       obs-stack-kube-state-metrics-7c989b94c7-jgzdp
162a99b29f941       quay.io/prometheus/node-exporter@sha256:337ff1d356b68d39cef853e8c6345de11ce7556bb34cda8bd205bcf2ed30b565                         24 minutes ago      Running             node-exporter             0                   d8b70cfe17ebd       obs-stack-prometheus-node-exporter-7nb8d
f9310723eaf57       kikyputraa/sast-scan-test@sha256:89931da0c121fc9351c94c73ed7c94c82269100faf9d02d79ebf00acaf78976b                                27 minutes ago      Running             flask-app                 0                   408a346cab5e5       flask-app-deployment-658844656-4qsm9
930d7005abb67       kikyputraa/sast-scan-test@sha256:89931da0c121fc9351c94c73ed7c94c82269100faf9d02d79ebf00acaf78976b                                27 minutes ago      Running             flask-app                 0                   78e8295cf0ddd       flask-app-deployment-658844656-sgkcv
c7153b7e079ae       6e38f40d628db                                                                                                                    28 minutes ago      Running             storage-provisioner       11                  dcd5294789fba       storage-provisioner
ea9bd9c48546e       kikyputraa/sast-scan-test@sha256:89931da0c121fc9351c94c73ed7c94c82269100faf9d02d79ebf00acaf78976b                                28 minutes ago      Running             flask-app                 1                   b1c2fe2dd3902       flask-app-deployment-747566b758-k2wps
454b2111f802b       kikyputraa/hello-devops@sha256:5ee937437d4c57501ce904b9ee918b42069a8608a405ee855751002fd11d9f69                                  28 minutes ago      Running             hello-devops              3                   58f8a41643a3f       hello-devops-677bf497c9-9ccpl
cf42c16a0af14       kikyputraa/sast-scan-test@sha256:89931da0c121fc9351c94c73ed7c94c82269100faf9d02d79ebf00acaf78976b                                28 minutes ago      Running             flask-app                 1                   4384c3a23fb2d       flask-app-deployment-747566b758-cjb5q
d21c5b330c5e0       1bec18b3728e7                                                                                                                    29 minutes ago      Running             controller                5                   0ee59920aac7f       ingress-nginx-controller-9cc49f96f-cjvxq
d97b4287f69df       df0860106674d                                                                                                                    29 minutes ago      Running             kube-proxy                6                   c8b89ce104bab       kube-proxy-bhhjb
eee775c0d4ffe       6e38f40d628db                                                                                                                    29 minutes ago      Exited              storage-provisioner       10                  dcd5294789fba       storage-provisioner
e6ff40ff51bfc       52546a367cc9e                                                                                                                    29 minutes ago      Running             coredns                   6                   b8b539e65daff       coredns-66bc5c9577-v8fzs
783ed8ba39e22       46169d968e920                                                                                                                    29 minutes ago      Running             kube-scheduler            6                   cce46a5469902       kube-scheduler-minikube
99a7aedbbdeb2       5f1f5298c888d                                                                                                                    29 minutes ago      Running             etcd                      6                   63ac72014878a       etcd-minikube
31bf7d6af563c       a0af72f2ec6d6                                                                                                                    29 minutes ago      Running             kube-controller-manager   6                   79664b19d2392       kube-controller-manager-minikube
4adaf2422bff7       90550c43ad2bc                                                                                                                    29 minutes ago      Running             kube-apiserver            6                   c6ea9729f9409       kube-apiserver-minikube
659d2bb0f7462       kikyputraa/sast-scan-test@sha256:24dedcc8cb29782b71126d218d41267eb31d095ada7cdaa1efa17235b43e1447                                2 days ago          Exited              flask-app                 0                   16fbfe0e53e5c       flask-app-deployment-747566b758-k2wps
8aef06e30fa27       kikyputraa/sast-scan-test@sha256:24dedcc8cb29782b71126d218d41267eb31d095ada7cdaa1efa17235b43e1447                                2 days ago          Exited              flask-app                 0                   e21cdf97bf672       flask-app-deployment-747566b758-cjb5q
d68c039e1c4a3       kikyputraa/hello-devops@sha256:5ee937437d4c57501ce904b9ee918b42069a8608a405ee855751002fd11d9f69                                  2 days ago          Exited              hello-devops              2                   8410a558978ef       hello-devops-677bf497c9-9ccpl
d69305de11dcc       df0860106674d                                                                                                                    2 days ago          Exited              kube-proxy                5                   5068a458d6d77       kube-proxy-bhhjb
0b72ec6fd8aba       46169d968e920                                                                                                                    2 days ago          Exited              kube-scheduler            5                   ce0f9fa92d94a       kube-scheduler-minikube
e17fd81d9f069       a0af72f2ec6d6                                                                                                                    2 days ago          Exited              kube-controller-manager   5                   71c1857dc89f2       kube-controller-manager-minikube
5009d579b1313       1bec18b3728e7                                                                                                                    2 days ago          Exited              controller                4                   66a0ebdbc9746       ingress-nginx-controller-9cc49f96f-cjvxq
8801e00adff66       52546a367cc9e                                                                                                                    2 days ago          Exited              coredns                   5                   3a3d55d3cb2b0       coredns-66bc5c9577-v8fzs
9c69f2f35913b       90550c43ad2bc                                                                                                                    2 days ago          Exited              kube-apiserver            5                   59bfadf56b1aa       kube-apiserver-minikube
6e33af37a50aa       5f1f5298c888d                                                                                                                    2 days ago          Exited              etcd                      5                   5aafccabddc37       etcd-minikube
a59c948b09f4f       8c217da6734db                                                                                                                    3 days ago          Exited              patch                     1                   b536b784a0e44       ingress-nginx-admission-patch-9hm6z
f13679846d1ba       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24       3 days ago          Exited              create                    0                   52502ec3fbc11       ingress-nginx-admission-create-g57cr


==> controller_ingress [5009d579b131] <==
W0109 07:07:09.033924       7 client_config.go:667] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0109 07:07:09.038966       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
I0109 07:07:09.054624       7 main.go:248] "Running in Kubernetes cluster" major="1" minor="34" git="v1.34.0" state="clean" commit="f28b4c9efbca5c5c0af716d9f2d5702667ee8a45" platform="linux/amd64"
I0109 07:07:09.326991       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0109 07:07:09.343526       7 ssl.go:535] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0109 07:07:09.359194       7 nginx.go:273] "Starting NGINX Ingress controller"
I0109 07:07:09.365090       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"76a9b603-9fd9-4cc5-9a28-dfb2c17fefbc", APIVersion:"v1", ResourceVersion:"517", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0109 07:07:09.366545       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"b683c45e-68da-4314-b856-7cefc2d5117b", APIVersion:"v1", ResourceVersion:"518", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0109 07:07:09.366635       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"9f5a8d7c-5707-404a-bad9-3e211e7295f3", APIVersion:"v1", ResourceVersion:"519", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0109 07:07:10.561712       7 leaderelection.go:257] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0109 07:07:10.561631       7 nginx.go:319] "Starting NGINX process"
I0109 07:07:10.562883       7 nginx.go:339] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0109 07:07:10.564979       7 controller.go:214] "Configuration changes detected, backend reload required"
I0109 07:07:10.574967       7 leaderelection.go:271] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0109 07:07:10.575091       7 status.go:85] "New leader elected" identity="ingress-nginx-controller-9cc49f96f-cjvxq"
I0109 07:07:10.578795       7 status.go:224] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-9cc49f96f-cjvxq" node="minikube"
I0109 07:07:10.617552       7 controller.go:228] "Backend successfully reloaded"
I0109 07:07:10.618374       7 controller.go:240] "Initial sync, sleeping for 1 second"
I0109 07:07:10.618478       7 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-9cc49f96f-cjvxq", UID:"b55b8175-5695-4061-b8cd-3ee7e93c6c0a", APIVersion:"v1", ResourceVersion:"12129", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I0109 07:07:10.680220       7 status.go:224] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-9cc49f96f-cjvxq" node="minikube"
I0109 08:34:37.669313       7 sigterm.go:36] "Received SIGTERM, shutting down"
I0109 08:34:37.670059       7 nginx.go:395] "Shutting down controller queues"
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.13.2
  Build:         11c69a64ce3c5bdfb6782434d9f62296d4b42179
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.27.1

-------------------------------------------------------------------------------



==> controller_ingress [d21c5b330c5e] <==
W0112 02:40:30.013282       7 client_config.go:667] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0112 02:40:30.015668       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
W0112 02:40:58.673855       7 main.go:245] Initial connection to the Kubernetes API server was retried 1 times.
I0112 02:40:58.673894       7 main.go:248] "Running in Kubernetes cluster" major="1" minor="34" git="v1.34.0" state="clean" commit="f28b4c9efbca5c5c0af716d9f2d5702667ee8a45" platform="linux/amd64"
I0112 02:40:58.708154       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0112 02:40:58.725889       7 ssl.go:535] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0112 02:40:58.738917       7 nginx.go:273] "Starting NGINX Ingress controller"
I0112 02:40:58.749471       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"76a9b603-9fd9-4cc5-9a28-dfb2c17fefbc", APIVersion:"v1", ResourceVersion:"517", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0112 02:40:58.749530       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"b683c45e-68da-4314-b856-7cefc2d5117b", APIVersion:"v1", ResourceVersion:"518", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0112 02:40:58.749597       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"9f5a8d7c-5707-404a-bad9-3e211e7295f3", APIVersion:"v1", ResourceVersion:"519", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0112 02:40:59.943122       7 nginx.go:319] "Starting NGINX process"
I0112 02:40:59.943312       7 leaderelection.go:257] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0112 02:40:59.943600       7 nginx.go:339] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0112 02:40:59.944246       7 controller.go:214] "Configuration changes detected, backend reload required"
I0112 02:40:59.950864       7 leaderelection.go:271] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0112 02:40:59.950952       7 status.go:85] "New leader elected" identity="ingress-nginx-controller-9cc49f96f-cjvxq"
I0112 02:40:59.953712       7 status.go:224] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-9cc49f96f-cjvxq" node="minikube"
I0112 02:40:59.956168       7 status.go:224] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-9cc49f96f-cjvxq" node="minikube"
I0112 02:41:00.002648       7 controller.go:228] "Backend successfully reloaded"
I0112 02:41:00.002828       7 controller.go:240] "Initial sync, sleeping for 1 second"
I0112 02:41:00.002908       7 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-9cc49f96f-cjvxq", UID:"b55b8175-5695-4061-b8cd-3ee7e93c6c0a", APIVersion:"v1", ResourceVersion:"17511", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.13.2
  Build:         11c69a64ce3c5bdfb6782434d9f62296d4b42179
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.27.1

-------------------------------------------------------------------------------



==> coredns [8801e00adff6] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:52986 - 37272 "HINFO IN 1642688249152299848.415278807846748420. udp 56 false 512" NXDOMAIN qr,rd 131 0.081804384s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [e6ff40ff51bf] <==
[INFO] 10.244.0.40:42486 - 33515 "A IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000489439s
[INFO] 10.244.0.40:51428 - 55735 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000740627s
[INFO] 10.244.0.40:55509 - 17805 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 176 0.000254792s
[INFO] 10.244.0.40:53359 - 40472 "A IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 164 0.000434028s
[INFO] 10.244.0.40:44744 - 48398 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000315804s
[INFO] 10.244.0.40:51331 - 13527 "A IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000527091s
[INFO] 10.244.0.40:40839 - 809 "A IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 164 0.000166819s
[INFO] 10.244.0.40:44360 - 48456 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 176 0.000227971s
[INFO] 10.244.0.40:50049 - 27372 "AAAA IN grafana.com.monitoring.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000307837s
[INFO] 10.244.0.40:45585 - 33388 "A IN grafana.com.monitoring.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000431629s
[INFO] 10.244.0.40:59715 - 110 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000143406s
[INFO] 10.244.0.40:48030 - 8594 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000142961s
[INFO] 10.244.0.40:38550 - 27272 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000124302s
[INFO] 10.244.0.40:50191 - 57314 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000173417s
[INFO] 10.244.0.40:54986 - 14777 "A IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd 33 0.007458639s
[INFO] 10.244.0.40:46459 - 20638 "AAAA IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd 33 0.008083354s
[INFO] 10.244.0.40:49897 - 1781 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd 68 0.010416018s
[INFO] 10.244.0.40:58674 - 31721 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd 56 0.010877078s
[INFO] 10.244.0.40:40820 - 31534 "A IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.00036023s
[INFO] 10.244.0.40:36460 - 28465 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000443124s
[INFO] 10.244.0.40:47727 - 51427 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 176 0.000161381s
[INFO] 10.244.0.40:56923 - 24344 "A IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 164 0.000269906s
[INFO] 10.244.0.40:57817 - 33050 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000263041s
[INFO] 10.244.0.40:44371 - 33071 "A IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.0003483s
[INFO] 10.244.0.40:36724 - 13354 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 176 0.000217475s
[INFO] 10.244.0.40:39487 - 23238 "A IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 164 0.000296809s
[INFO] 10.244.0.40:59638 - 65333 "A IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000417779s
[INFO] 10.244.0.40:33055 - 24023 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000562059s
[INFO] 10.244.0.40:52078 - 11871 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 176 0.000253826s
[INFO] 10.244.0.40:57239 - 17489 "A IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 164 0.000281619s
[INFO] 10.244.0.40:50244 - 44338 "A IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000328699s
[INFO] 10.244.0.40:37597 - 51202 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000449099s
[INFO] 10.244.0.40:34024 - 39272 "A IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 164 0.000157853s
[INFO] 10.244.0.40:59503 - 42430 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 176 0.000172686s
[INFO] 10.244.0.40:42320 - 29502 "A IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000267709s
[INFO] 10.244.0.40:40604 - 49858 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000397188s
[INFO] 10.244.0.40:44685 - 40014 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 176 0.000116097s
[INFO] 10.244.0.40:50329 - 278 "A IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 164 0.000127841s
[INFO] 10.244.0.40:49420 - 63114 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000340931s
[INFO] 10.244.0.40:38711 - 44945 "A IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000423618s
[INFO] 10.244.0.40:50467 - 6951 "A IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 164 0.000195571s
[INFO] 10.244.0.40:35942 - 63661 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 176 0.000285423s
[INFO] 10.244.0.40:52307 - 30664 "AAAA IN grafana.com.monitoring.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000362423s
[INFO] 10.244.0.40:36627 - 47285 "A IN grafana.com.monitoring.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000454824s
[INFO] 10.244.0.40:42974 - 38254 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000164377s
[INFO] 10.244.0.40:40465 - 1003 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000213373s
[INFO] 10.244.0.40:57200 - 53004 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000127451s
[INFO] 10.244.0.40:39524 - 39863 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000178704s
[INFO] 10.244.0.40:49341 - 49287 "A IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd 33 0.008763042s
[INFO] 10.244.0.40:56809 - 57688 "AAAA IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd 33 0.008977025s
[INFO] 10.244.0.40:44366 - 36932 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd 68 0.010129766s
[INFO] 10.244.0.40:37331 - 32104 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd 56 0.021107842s
[INFO] 10.244.0.40:32951 - 64622 "A IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000425757s
[INFO] 10.244.0.40:44796 - 6153 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000486465s
[INFO] 10.244.0.40:57664 - 60648 "A IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 164 0.000444931s
[INFO] 10.244.0.40:41923 - 24706 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 176 0.000543432s
[INFO] 10.244.0.40:36522 - 13774 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.00027827s
[INFO] 10.244.0.40:36997 - 34670 "A IN obs-stack-kube-prometheus-prometheus.monitoring.monitoring.svc.cluster.local. udp 105 false 1232" NXDOMAIN qr,aa,rd 187 0.000346097s
[INFO] 10.244.0.40:40898 - 55654 "AAAA IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 176 0.000151036s
[INFO] 10.244.0.40:35726 - 5911 "A IN obs-stack-kube-prometheus-prometheus.monitoring.svc.cluster.local. udp 94 false 1232" NOERROR qr,aa,rd 164 0.000264192s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2026_01_08T14_26_29_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 08 Jan 2026 07:26:26 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 12 Jan 2026 03:09:20 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 12 Jan 2026 03:04:55 +0000   Thu, 08 Jan 2026 07:26:24 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 12 Jan 2026 03:04:55 +0000   Thu, 08 Jan 2026 07:26:24 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 12 Jan 2026 03:04:55 +0000   Thu, 08 Jan 2026 07:26:24 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 12 Jan 2026 03:04:55 +0000   Thu, 08 Jan 2026 07:26:26 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7733436Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7733436Ki
  pods:               110
System Info:
  Machine ID:                 bfa68f150e3b4decaffe1fc43bb53dfa
  System UUID:                bfa68f150e3b4decaffe1fc43bb53dfa
  Boot ID:                    42696693-398e-4d45-bd74-4c29c1fcd9b6
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (20 in total)
  Namespace                   Name                                                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                     ------------  ----------  ---------------  -------------  ---
  default                     flask-app-deployment-747566b758-cjb5q                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d19h
  default                     flask-app-deployment-747566b758-k2wps                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d19h
  default                     hello-devops-677bf497c9-9ccpl                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d18h
  ingress-nginx               ingress-nginx-controller-9cc49f96f-cjvxq                 100m (0%)     0 (0%)      90Mi (1%)        0 (0%)         3d19h
  kube-system                 coredns-66bc5c9577-v8fzs                                 100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     3d19h
  kube-system                 etcd-minikube                                            100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         3d19h
  kube-system                 kube-apiserver-minikube                                  250m (1%)     0 (0%)      0 (0%)           0 (0%)         3d19h
  kube-system                 kube-controller-manager-minikube                         200m (1%)     0 (0%)      0 (0%)           0 (0%)         3d19h
  kube-system                 kube-proxy-bhhjb                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d19h
  kube-system                 kube-scheduler-minikube                                  100m (0%)     0 (0%)      0 (0%)           0 (0%)         3d19h
  kube-system                 metrics-server-85b7d694d7-7pqsn                          100m (0%)     0 (0%)      200Mi (2%)       0 (0%)         8m41s
  kube-system                 storage-provisioner                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d19h
  monitoring                  alertmanager-obs-stack-kube-prometheus-alertmanager-0    0 (0%)        0 (0%)      200Mi (2%)       0 (0%)         24m
  monitoring                  obs-stack-grafana-74958449f-szw2h                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m
  monitoring                  obs-stack-kube-prometheus-operator-64587f75b5-8xwm8      0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m
  monitoring                  obs-stack-kube-state-metrics-7c989b94c7-jgzdp            0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m
  monitoring                  obs-stack-prometheus-node-exporter-7nb8d                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m
  monitoring                  prometheus-obs-stack-kube-prometheus-prometheus-0        0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m
  staging                     flask-app-deployment-658844656-4qsm9                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m
  staging                     flask-app-deployment-658844656-sgkcv                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (5%)   0 (0%)
  memory             660Mi (8%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                From             Message
  ----     ------                   ----               ----             -------
  Normal   Starting                 28m                kube-proxy       
  Normal   Starting                 29m                kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  29m (x8 over 29m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    29m (x8 over 29m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     29m (x7 over 29m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  29m                kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 29m                kubelet          Node minikube has been rebooted, boot id: 42696693-398e-4d45-bd74-4c29c1fcd9b6
  Normal   RegisteredNode           29m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Jan12 02:28] Speculative Return Stack Overflow: IBPB-extending microcode not applied!
[  +0.000001] Speculative Return Stack Overflow: WARNING: See https://kernel.org/doc/html/latest/admin-guide/hw-vuln/srso.html for mitigation options.
[  +0.060115] PCI: Fatal: No config space access function found
[  +0.048309] PCI: System does not support PCI
[  +0.173275] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +2.397766] Failed to connect to bus: No such file or directory
[  +0.059034] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.005829] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000743] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000554] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000786] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.278592] systemd-journald[46]: File /var/log/journal/0f1ada86c65c464a8e14ae34f0e89c1f/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.842694] weston[294]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +1.107121] TCP: eth0: Driver has suspect GRO implementation, TCP performance may be compromised.
[  +1.358849] WSL (267) ERROR: CheckConnection: getaddrinfo() failed: -5


==> etcd [6e33af37a50a] <==
{"level":"info","ts":"2026-01-09T07:32:28.510172Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":13617}
{"level":"info","ts":"2026-01-09T07:32:28.517573Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":13617,"took":"7.045465ms","hash":4065099588,"current-db-size-bytes":3481600,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1687552,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2026-01-09T07:32:28.517676Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":4065099588,"revision":13617,"compact-revision":13336}
{"level":"info","ts":"2026-01-09T07:37:34.075607Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":13896}
{"level":"info","ts":"2026-01-09T07:37:34.081728Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":13896,"took":"5.7548ms","hash":2780995522,"current-db-size-bytes":3481600,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1646592,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-09T07:37:34.081764Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2780995522,"revision":13896,"compact-revision":13617}
{"level":"info","ts":"2026-01-09T07:41:53.386385Z","caller":"traceutil/trace.go:172","msg":"trace[2104904690] transaction","detail":"{read_only:false; response_revision:14416; number_of_response:1; }","duration":"113.472414ms","start":"2026-01-09T07:41:53.272531Z","end":"2026-01-09T07:41:53.386003Z","steps":["trace[2104904690] 'process raft request'  (duration: 113.28489ms)"],"step_count":1}
{"level":"info","ts":"2026-01-09T07:42:38.393458Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":14178}
{"level":"info","ts":"2026-01-09T07:42:38.404914Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":14178,"took":"10.993951ms","hash":2606504398,"current-db-size-bytes":3481600,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1605632,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-09T07:42:38.404983Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2606504398,"revision":14178,"compact-revision":13896}
{"level":"info","ts":"2026-01-09T07:47:38.948385Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":14457}
{"level":"info","ts":"2026-01-09T07:47:38.954721Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":14457,"took":"5.895114ms","hash":3548066156,"current-db-size-bytes":3481600,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1601536,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-09T07:47:38.954765Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3548066156,"revision":14457,"compact-revision":14178}
{"level":"info","ts":"2026-01-09T07:52:38.961519Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":14737}
{"level":"info","ts":"2026-01-09T07:52:38.967658Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":14737,"took":"5.736036ms","hash":3393174554,"current-db-size-bytes":3481600,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1724416,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2026-01-09T07:52:38.967718Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3393174554,"revision":14737,"compact-revision":14457}
{"level":"info","ts":"2026-01-09T07:57:38.974854Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":15059}
{"level":"info","ts":"2026-01-09T07:57:38.982119Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":15059,"took":"6.902279ms","hash":2943771696,"current-db-size-bytes":3481600,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1769472,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2026-01-09T07:57:38.982187Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2943771696,"revision":15059,"compact-revision":14737}
{"level":"info","ts":"2026-01-09T08:02:39.396582Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":15339}
{"level":"info","ts":"2026-01-09T08:02:39.403518Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":15339,"took":"6.431981ms","hash":3515425148,"current-db-size-bytes":3481600,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1671168,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2026-01-09T08:02:39.403580Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3515425148,"revision":15339,"compact-revision":15059}
{"level":"info","ts":"2026-01-09T08:07:39.410584Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":15619}
{"level":"info","ts":"2026-01-09T08:07:39.417391Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":15619,"took":"6.527785ms","hash":1892080836,"current-db-size-bytes":3481600,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1687552,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2026-01-09T08:07:39.417461Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1892080836,"revision":15619,"compact-revision":15339}
{"level":"info","ts":"2026-01-09T08:12:39.426634Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":15899}
{"level":"info","ts":"2026-01-09T08:12:39.432780Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":15899,"took":"5.841803ms","hash":1433807674,"current-db-size-bytes":3481600,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1732608,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2026-01-09T08:12:39.432819Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1433807674,"revision":15899,"compact-revision":15619}
{"level":"info","ts":"2026-01-09T08:17:40.463804Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":16186}
{"level":"info","ts":"2026-01-09T08:17:40.471609Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":16186,"took":"7.518559ms","hash":3248391749,"current-db-size-bytes":3481600,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1843200,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2026-01-09T08:17:40.471679Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3248391749,"revision":16186,"compact-revision":15899}
{"level":"info","ts":"2026-01-09T08:20:32.784878Z","caller":"etcdserver/server.go:2185","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000,"snapshot-forced":false}
{"level":"info","ts":"2026-01-09T08:20:32.800063Z","caller":"etcdserver/server.go:2230","msg":"saved snapshot to disk","snapshot-index":20002}
{"level":"info","ts":"2026-01-09T08:22:40.481064Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":16472}
{"level":"info","ts":"2026-01-09T08:22:40.491092Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":16472,"took":"9.433284ms","hash":4161650269,"current-db-size-bytes":3481600,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1724416,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2026-01-09T08:22:40.491210Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":4161650269,"revision":16472,"compact-revision":16186}
{"level":"info","ts":"2026-01-09T08:27:40.495197Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":16752}
{"level":"info","ts":"2026-01-09T08:27:40.502373Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":16752,"took":"6.972219ms","hash":3700459063,"current-db-size-bytes":3481600,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1638400,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-09T08:27:40.502468Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3700459063,"revision":16752,"compact-revision":16472}
{"level":"info","ts":"2026-01-09T08:32:40.511038Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":17032}
{"level":"info","ts":"2026-01-09T08:32:40.517285Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":17032,"took":"5.999364ms","hash":2982954312,"current-db-size-bytes":3481600,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1642496,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-09T08:32:40.517346Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2982954312,"revision":17032,"compact-revision":16752}
{"level":"info","ts":"2026-01-09T08:34:37.616451Z","caller":"osutil/interrupt_unix.go:65","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2026-01-09T08:34:37.617563Z","caller":"embed/etcd.go:426","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"error","ts":"2026-01-09T08:34:37.621437Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"http: Server closed","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*serveCtx).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/serve.go:90"}
{"level":"error","ts":"2026-01-09T08:34:44.624087Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"http: Server closed","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*serveCtx).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/serve.go:90"}
{"level":"error","ts":"2026-01-09T08:34:44.624292Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 127.0.0.1:2381: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"warn","ts":"2026-01-09T08:34:44.628426Z","caller":"embed/serve.go:245","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2026-01-09T08:34:44.628966Z","caller":"embed/serve.go:245","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"info","ts":"2026-01-09T08:34:44.626566Z","caller":"etcdserver/server.go:1281","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"warn","ts":"2026-01-09T08:34:44.629017Z","caller":"embed/serve.go:247","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2026-01-09T08:34:44.629084Z","caller":"embed/serve.go:247","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"error","ts":"2026-01-09T08:34:44.629825Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 127.0.0.1:2379: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2026-01-09T08:34:44.629861Z","caller":"etcdserver/server.go:2342","msg":"server has stopped; stopping storage version's monitor"}
{"level":"error","ts":"2026-01-09T08:34:44.629820Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 192.168.49.2:2379: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2026-01-09T08:34:44.629803Z","caller":"etcdserver/server.go:2319","msg":"server has stopped; stopping cluster version's monitor"}
{"level":"info","ts":"2026-01-09T08:34:44.645421Z","caller":"embed/etcd.go:621","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"error","ts":"2026-01-09T08:34:44.645768Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 192.168.49.2:2380: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2026-01-09T08:34:44.646019Z","caller":"embed/etcd.go:626","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2026-01-09T08:34:44.646089Z","caller":"embed/etcd.go:428","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [99a7aedbbdeb] <==
{"level":"warn","ts":"2026-01-12T02:53:43.004084Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40710","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T02:54:10.529777Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35826","server-name":"","error":"EOF"}
{"level":"info","ts":"2026-01-12T02:54:16.985354Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":18663}
{"level":"info","ts":"2026-01-12T02:54:17.045148Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":18663,"took":"59.148603ms","hash":3503525701,"current-db-size-bytes":20979712,"current-db-size":"21 MB","current-db-size-in-use-bytes":5431296,"current-db-size-in-use":"5.4 MB"}
{"level":"info","ts":"2026-01-12T02:54:17.045248Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3503525701,"revision":18663,"compact-revision":18256}
{"level":"warn","ts":"2026-01-12T02:54:38.028730Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:60458","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T02:55:05.541348Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58958","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T02:55:33.107291Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54116","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T02:56:03.107843Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:51278","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T02:56:30.579700Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42872","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T02:56:58.124171Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:60730","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T02:57:25.610178Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:39458","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T02:57:53.136508Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34486","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T02:58:20.629748Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49378","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T02:58:48.162133Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:47662","server-name":"","error":"EOF"}
{"level":"info","ts":"2026-01-12T02:58:54.637134Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":18939}
{"level":"info","ts":"2026-01-12T02:58:54.644696Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":18939,"took":"7.213476ms","hash":2014805074,"current-db-size-bytes":20979712,"current-db-size":"21 MB","current-db-size-in-use-bytes":5873664,"current-db-size-in-use":"5.9 MB"}
{"level":"info","ts":"2026-01-12T02:58:54.644743Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2014805074,"revision":18939,"compact-revision":18663}
{"level":"info","ts":"2026-01-12T02:59:04.061223Z","caller":"traceutil/trace.go:172","msg":"trace[280037877] transaction","detail":"{read_only:false; response_revision:19223; number_of_response:1; }","duration":"143.731224ms","start":"2026-01-12T02:59:03.883461Z","end":"2026-01-12T02:59:04.027192Z","steps":["trace[280037877] 'process raft request'  (duration: 143.563234ms)"],"step_count":1}
{"level":"info","ts":"2026-01-12T02:59:10.310123Z","caller":"traceutil/trace.go:172","msg":"trace[1322263537] linearizableReadLoop","detail":"{readStateIndex:23043; appliedIndex:23043; }","duration":"277.97685ms","start":"2026-01-12T02:59:10.032017Z","end":"2026-01-12T02:59:10.309994Z","steps":["trace[1322263537] 'read index received'  (duration: 277.968341ms)","trace[1322263537] 'applied index is now lower than readState.Index'  (duration: 6.61¬µs)"],"step_count":2}
{"level":"warn","ts":"2026-01-12T02:59:11.145901Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"1.10859381s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2026-01-12T02:59:11.146259Z","caller":"traceutil/trace.go:172","msg":"trace[141008782] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:19229; }","duration":"1.114183375s","start":"2026-01-12T02:59:10.032009Z","end":"2026-01-12T02:59:11.146192Z","steps":["trace[141008782] 'agreement among raft nodes before linearized reading'  (duration: 278.188789ms)","trace[141008782] 'range keys from in-memory index tree'  (duration: 830.320205ms)"],"step_count":2}
{"level":"warn","ts":"2026-01-12T02:59:11.149170Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"830.723694ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128042608040469993 > lease_revoke:<id:70cc9bb0137de593>","response":"size:30"}
{"level":"info","ts":"2026-01-12T02:59:11.149274Z","caller":"traceutil/trace.go:172","msg":"trace[113083073] linearizableReadLoop","detail":"{readStateIndex:23044; appliedIndex:23043; }","duration":"838.987966ms","start":"2026-01-12T02:59:10.310275Z","end":"2026-01-12T02:59:11.149263Z","steps":["trace[113083073] 'read index received'  (duration: 40.692¬µs)","trace[113083073] 'applied index is now lower than readState.Index'  (duration: 838.946698ms)"],"step_count":2}
{"level":"warn","ts":"2026-01-12T02:59:11.149351Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"825.187336ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2026-01-12T02:59:11.149382Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"1.026126832s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1111"}
{"level":"info","ts":"2026-01-12T02:59:11.149382Z","caller":"traceutil/trace.go:172","msg":"trace[360380544] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:19229; }","duration":"825.222775ms","start":"2026-01-12T02:59:10.324154Z","end":"2026-01-12T02:59:11.149377Z","steps":["trace[360380544] 'agreement among raft nodes before linearized reading'  (duration: 825.16972ms)"],"step_count":1}
{"level":"info","ts":"2026-01-12T02:59:11.149418Z","caller":"traceutil/trace.go:172","msg":"trace[1211509697] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:19229; }","duration":"1.026165581s","start":"2026-01-12T02:59:10.123243Z","end":"2026-01-12T02:59:11.149408Z","steps":["trace[1211509697] 'agreement among raft nodes before linearized reading'  (duration: 1.026046543s)"],"step_count":1}
{"level":"warn","ts":"2026-01-12T02:59:11.149496Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-01-12T02:59:10.324129Z","time spent":"825.335908ms","remote":"127.0.0.1:52326","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2026-01-12T02:59:11.149502Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-01-12T02:59:10.123223Z","time spent":"1.026249909s","remote":"127.0.0.1:52622","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":1135,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"warn","ts":"2026-01-12T02:59:15.671697Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52954","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T02:59:43.165477Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49696","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:00:10.673041Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43396","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:00:38.163565Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59438","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:01:05.678783Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:51802","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:01:33.228502Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44182","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:02:03.229349Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48382","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:02:30.721260Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:47144","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:02:58.229918Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45166","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:03:25.753502Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41022","server-name":"","error":"EOF"}
{"level":"info","ts":"2026-01-12T03:03:32.241852Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":19213}
{"level":"info","ts":"2026-01-12T03:03:32.249753Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":19213,"took":"7.394818ms","hash":233326205,"current-db-size-bytes":20979712,"current-db-size":"21 MB","current-db-size-in-use-bytes":6025216,"current-db-size-in-use":"6.0 MB"}
{"level":"info","ts":"2026-01-12T03:03:32.249805Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":233326205,"revision":19213,"compact-revision":18939}
{"level":"warn","ts":"2026-01-12T03:03:53.265947Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:38718","server-name":"","error":"EOF"}
{"level":"info","ts":"2026-01-12T03:04:06.817721Z","caller":"traceutil/trace.go:172","msg":"trace[2039032723] transaction","detail":"{read_only:false; response_revision:19574; number_of_response:1; }","duration":"126.402633ms","start":"2026-01-12T03:04:06.691284Z","end":"2026-01-12T03:04:06.817687Z","steps":["trace[2039032723] 'process raft request'  (duration: 126.263678ms)"],"step_count":1}
{"level":"warn","ts":"2026-01-12T03:04:19.612544Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52014","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:04:47.161853Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:38418","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:05:14.629081Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52560","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:05:44.628296Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46796","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:06:12.187690Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34422","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:06:39.690210Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50390","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:07:07.180278Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48354","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:07:34.730253Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59634","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:08:02.213783Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50142","server-name":"","error":"EOF"}
{"level":"info","ts":"2026-01-12T03:08:08.723986Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":19540}
{"level":"info","ts":"2026-01-12T03:08:08.758557Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":19540,"took":"34.071591ms","hash":1037530734,"current-db-size-bytes":20979712,"current-db-size":"21 MB","current-db-size-in-use-bytes":5464064,"current-db-size-in-use":"5.5 MB"}
{"level":"info","ts":"2026-01-12T03:08:08.758613Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1037530734,"revision":19540,"compact-revision":19213}
{"level":"warn","ts":"2026-01-12T03:08:29.713012Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37700","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:08:57.239050Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57802","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-01-12T03:09:24.729780Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56926","server-name":"","error":"EOF"}


==> kernel <==
 03:09:30 up 41 min,  0 users,  load average: 0.33, 0.28, 0.28
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [4adaf2422bff] <==
E0112 03:01:45.796782       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0112 03:01:45.796791       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0112 03:01:57.388536       1 remote_available_controller.go:462] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.105.58.55:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.105.58.55:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.105.58.55:443: connect: connection refused" logger="UnhandledError"
W0112 03:01:57.388602       1 handler_proxy.go:99] no RequestInfo found in the context
E0112 03:01:57.388633       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0112 03:01:57.388967       1 remote_available_controller.go:462] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.105.58.55:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.105.58.55:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.105.58.55:443: connect: connection refused" logger="UnhandledError"
I0112 03:01:57.878018       1 stats.go:136] "Error getting keys" err="empty key: \"\""
W0112 03:01:58.390449       1 handler_proxy.go:99] no RequestInfo found in the context
E0112 03:01:58.390523       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0112 03:01:58.390540       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0112 03:01:58.390471       1 handler_proxy.go:99] no RequestInfo found in the context
E0112 03:01:58.390586       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0112 03:01:58.392042       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0112 03:02:02.404247       1 handler_proxy.go:99] no RequestInfo found in the context
E0112 03:02:02.404348       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0112 03:02:02.404379       1 remote_available_controller.go:462] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.105.58.55:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.105.58.55:443/apis/metrics.k8s.io/v1beta1\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" logger="UnhandledError"
I0112 03:02:02.411597       1 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0112 03:02:03.228587       1 logging.go:55] [core] [Channel #641 SubChannel #642]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I0112 03:02:18.055221       1 stats.go:136] "Error getting keys" err="empty key: \"\""
W0112 03:02:30.721132       1 logging.go:55] [core] [Channel #651 SubChannel #652]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W0112 03:02:58.229730       1 logging.go:55] [core] [Channel #661 SubChannel #662]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I0112 03:03:00.276060       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0112 03:03:24.859806       1 stats.go:136] "Error getting keys" err="empty key: \"\""
W0112 03:03:25.753384       1 logging.go:55] [core] [Channel #671 SubChannel #672]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W0112 03:03:53.265791       1 logging.go:55] [core] [Channel #678 SubChannel #679]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I0112 03:04:15.780347       1 stats.go:136] "Error getting keys" err="empty key: \"\""
W0112 03:04:19.612377       1 logging.go:55] [core] [Channel #688 SubChannel #689]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I0112 03:04:34.448492       1 stats.go:136] "Error getting keys" err="empty key: \"\""
W0112 03:04:47.161689       1 logging.go:55] [core] [Channel #698 SubChannel #699]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W0112 03:05:14.628952       1 logging.go:55] [core] [Channel #708 SubChannel #709]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I0112 03:05:15.990450       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0112 03:05:38.559360       1 stats.go:136] "Error getting keys" err="empty key: \"\""
W0112 03:05:44.628227       1 logging.go:55] [core] [Channel #718 SubChannel #719]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W0112 03:06:12.187599       1 logging.go:55] [core] [Channel #728 SubChannel #729]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I0112 03:06:24.314072       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0112 03:06:33.952812       1 stats.go:136] "Error getting keys" err="empty key: \"\""
W0112 03:06:39.690122       1 logging.go:55] [core] [Channel #735 SubChannel #736]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W0112 03:07:07.180098       1 logging.go:55] [core] [Channel #745 SubChannel #746]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I0112 03:07:25.549408       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0112 03:07:30.451904       1 stats.go:136] "Error getting keys" err="empty key: \"\""
W0112 03:07:34.730041       1 logging.go:55] [core] [Channel #755 SubChannel #756]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W0112 03:08:02.213618       1 logging.go:55] [core] [Channel #765 SubChannel #766]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I0112 03:08:09.660220       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0112 03:08:23.339819       1 stats.go:136] "Error getting keys" err="empty key: \"\""
W0112 03:08:29.712949       1 logging.go:55] [core] [Channel #772 SubChannel #773]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I0112 03:08:52.201710       1 stats.go:136] "Error getting keys" err="empty key: \"\""
W0112 03:08:57.239291       1 logging.go:55] [core] [Channel #782 SubChannel #783]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W0112 03:09:24.729680       1 logging.go:55] [core] [Channel #792 SubChannel #793]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"


==> kube-apiserver [9c69f2f35913] <==
W0109 08:34:42.964448       1 logging.go:55] [core] [Channel #171 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:42.967352       1 logging.go:55] [core] [Channel #79 SubChannel #81]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:42.994372       1 logging.go:55] [core] [Channel #191 SubChannel #193]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.004184       1 logging.go:55] [core] [Channel #223 SubChannel #225]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.007869       1 logging.go:55] [core] [Channel #107 SubChannel #109]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.009337       1 logging.go:55] [core] [Channel #47 SubChannel #49]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.053881       1 logging.go:55] [core] [Channel #27 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.067741       1 logging.go:55] [core] [Channel #103 SubChannel #105]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.077281       1 logging.go:55] [core] [Channel #159 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.085169       1 logging.go:55] [core] [Channel #215 SubChannel #217]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.105773       1 logging.go:55] [core] [Channel #211 SubChannel #213]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.129578       1 logging.go:55] [core] [Channel #2 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.132316       1 logging.go:55] [core] [Channel #7 SubChannel #9]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.173364       1 logging.go:55] [core] [Channel #255 SubChannel #257]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.177241       1 logging.go:55] [core] [Channel #83 SubChannel #85]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.210397       1 logging.go:55] [core] [Channel #123 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.214209       1 logging.go:55] [core] [Channel #115 SubChannel #117]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.270427       1 logging.go:55] [core] [Channel #231 SubChannel #233]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.270441       1 logging.go:55] [core] [Channel #31 SubChannel #33]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.284291       1 logging.go:55] [core] [Channel #39 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.300299       1 logging.go:55] [core] [Channel #219 SubChannel #221]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.308225       1 logging.go:55] [core] [Channel #195 SubChannel #197]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.315013       1 logging.go:55] [core] [Channel #11 SubChannel #13]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.329395       1 logging.go:55] [core] [Channel #71 SubChannel #73]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.329467       1 logging.go:55] [core] [Channel #203 SubChannel #205]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.367319       1 logging.go:55] [core] [Channel #167 SubChannel #169]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.385067       1 logging.go:55] [core] [Channel #91 SubChannel #93]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.483049       1 logging.go:55] [core] [Channel #235 SubChannel #237]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.483465       1 logging.go:55] [core] [Channel #43 SubChannel #45]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.545050       1 logging.go:55] [core] [Channel #239 SubChannel #241]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:43.563273       1 logging.go:55] [core] [Channel #131 SubChannel #133]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:45.631449       1 logging.go:55] [core] [Channel #155 SubChannel #157]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:45.670987       1 logging.go:55] [core] [Channel #35 SubChannel #37]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:45.751093       1 logging.go:55] [core] [Channel #59 SubChannel #61]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:45.853113       1 logging.go:55] [core] [Channel #22 SubChannel #24]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:45.958176       1 logging.go:55] [core] [Channel #63 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.012617       1 logging.go:55] [core] [Channel #247 SubChannel #249]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.030505       1 logging.go:55] [core] [Channel #175 SubChannel #177]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.241270       1 logging.go:55] [core] [Channel #135 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.292098       1 logging.go:55] [core] [Channel #127 SubChannel #129]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.388008       1 logging.go:55] [core] [Channel #147 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.423638       1 logging.go:55] [core] [Channel #179 SubChannel #181]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.429275       1 logging.go:55] [core] [Channel #2 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.434679       1 logging.go:55] [core] [Channel #143 SubChannel #145]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.472187       1 logging.go:55] [core] [Channel #243 SubChannel #245]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.492914       1 logging.go:55] [core] [Channel #1 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.521974       1 logging.go:55] [core] [Channel #171 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.526510       1 logging.go:55] [core] [Channel #55 SubChannel #57]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.552156       1 logging.go:55] [core] [Channel #211 SubChannel #213]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.643913       1 logging.go:55] [core] [Channel #119 SubChannel #121]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.741080       1 logging.go:55] [core] [Channel #255 SubChannel #257]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.745049       1 logging.go:55] [core] [Channel #195 SubChannel #197]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.752729       1 logging.go:55] [core] [Channel #7 SubChannel #9]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.763409       1 logging.go:55] [core] [Channel #183 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.763701       1 logging.go:55] [core] [Channel #139 SubChannel #141]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.791288       1 logging.go:55] [core] [Channel #79 SubChannel #81]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.817388       1 logging.go:55] [core] [Channel #207 SubChannel #209]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.863237       1 logging.go:55] [core] [Channel #103 SubChannel #105]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.896430       1 logging.go:55] [core] [Channel #123 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0109 08:34:46.914423       1 logging.go:55] [core] [Channel #91 SubChannel #93]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [31bf7d6af563] <==
I0112 02:40:30.854060       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I0112 02:40:30.854103       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I0112 02:40:30.854149       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I0112 02:40:30.854435       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I0112 02:40:30.854685       1 shared_informer.go:356] "Caches are synced" controller="GC"
I0112 02:40:30.854836       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I0112 02:40:30.854959       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I0112 02:40:30.855028       1 shared_informer.go:356] "Caches are synced" controller="taint"
I0112 02:40:30.855306       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I0112 02:40:30.855325       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0112 02:40:30.855628       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0112 02:40:30.855712       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0112 02:40:30.859295       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I0112 02:40:30.864653       1 shared_informer.go:356] "Caches are synced" controller="expand"
I0112 02:40:30.868345       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I0112 02:40:30.870861       1 shared_informer.go:356] "Caches are synced" controller="service account"
I0112 02:40:30.874471       1 shared_informer.go:356] "Caches are synced" controller="job"
I0112 02:40:30.879067       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I0112 02:40:30.881520       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I0112 02:40:30.885206       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I0112 02:40:30.887797       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I0112 02:40:30.890692       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I0112 02:40:30.894267       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I0112 02:40:30.904141       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I0112 02:40:30.904183       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I0112 02:40:30.904204       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I0112 02:40:30.904220       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I0112 02:40:30.904447       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I0112 02:40:30.908707       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0112 02:40:30.908737       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I0112 02:40:30.908769       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I0112 02:40:30.909406       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I0112 02:40:30.910298       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0112 02:40:30.911158       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I0112 02:40:30.914460       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I0112 02:40:30.915947       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I0112 02:40:30.954067       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0112 02:40:30.954097       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0112 02:40:30.954106       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0112 02:40:31.016615       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0112 02:44:38.592195       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="probes.monitoring.coreos.com"
I0112 02:44:38.592259       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="prometheusagents.monitoring.coreos.com"
I0112 02:44:38.592290       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="prometheusrules.monitoring.coreos.com"
I0112 02:44:38.592309       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="podmonitors.monitoring.coreos.com"
I0112 02:44:38.592332       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="prometheuses.monitoring.coreos.com"
I0112 02:44:38.592378       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="thanosrulers.monitoring.coreos.com"
I0112 02:44:38.592397       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="alertmanagerconfigs.monitoring.coreos.com"
I0112 02:44:38.592408       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="scrapeconfigs.monitoring.coreos.com"
I0112 02:44:38.592442       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="alertmanagers.monitoring.coreos.com"
I0112 02:44:38.592480       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="servicemonitors.monitoring.coreos.com"
I0112 02:44:38.592584       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I0112 02:44:38.706909       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I0112 02:44:39.793686       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0112 02:44:39.807364       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
E0112 03:00:50.356811       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0112 03:00:50.361667       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0112 03:01:17.882597       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0112 03:01:17.886478       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0112 03:01:45.429424       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0112 03:01:45.434389       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"


==> kube-controller-manager [e17fd81d9f06] <==
I0109 07:07:12.569849       1 controller.go:173] "Starting ephemeral volume controller" logger="ephemeral-volume-controller"
I0109 07:07:12.569865       1 shared_informer.go:349] "Waiting for caches to sync" controller="ephemeral"
I0109 07:07:12.620771       1 controllermanager.go:781] "Started controller" controller="replicationcontroller-controller"
I0109 07:07:12.620961       1 replica_set.go:243] "Starting controller" logger="replicationcontroller-controller" name="replicationcontroller"
I0109 07:07:12.620997       1 shared_informer.go:349] "Waiting for caches to sync" controller="ReplicationController"
I0109 07:07:12.628606       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I0109 07:07:12.633799       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0109 07:07:12.646159       1 shared_informer.go:356] "Caches are synced" controller="job"
I0109 07:07:12.649380       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I0109 07:07:12.653731       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I0109 07:07:12.657337       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I0109 07:07:12.659667       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I0109 07:07:12.659949       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I0109 07:07:12.662471       1 shared_informer.go:356] "Caches are synced" controller="GC"
I0109 07:07:12.664055       1 shared_informer.go:356] "Caches are synced" controller="service account"
I0109 07:07:12.666443       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I0109 07:07:12.670050       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I0109 07:07:12.670089       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I0109 07:07:12.670149       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I0109 07:07:12.670417       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I0109 07:07:12.670487       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I0109 07:07:12.670508       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I0109 07:07:12.670516       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I0109 07:07:12.671372       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I0109 07:07:12.673581       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I0109 07:07:12.674260       1 shared_informer.go:356] "Caches are synced" controller="node"
I0109 07:07:12.674443       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0109 07:07:12.674630       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0109 07:07:12.674658       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I0109 07:07:12.674667       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I0109 07:07:12.679386       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I0109 07:07:12.681818       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I0109 07:07:12.684248       1 shared_informer.go:356] "Caches are synced" controller="taint"
I0109 07:07:12.684673       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0109 07:07:12.684993       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0109 07:07:12.685140       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0109 07:07:12.685880       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0109 07:07:12.688358       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I0109 07:07:12.691537       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I0109 07:07:12.692838       1 shared_informer.go:356] "Caches are synced" controller="expand"
I0109 07:07:12.696040       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I0109 07:07:12.719618       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I0109 07:07:12.719673       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I0109 07:07:12.719711       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I0109 07:07:12.719763       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I0109 07:07:12.720105       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I0109 07:07:12.720303       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0109 07:07:12.720472       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I0109 07:07:12.720485       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0109 07:07:12.720738       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I0109 07:07:12.721410       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0109 07:07:12.721449       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0109 07:07:12.721494       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I0109 07:07:12.727039       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0109 07:07:12.729261       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0109 07:07:12.736678       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I0109 07:07:12.743170       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0109 07:07:12.743209       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0109 07:07:12.743220       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0109 07:07:12.754409       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"


==> kube-proxy [d69305de11dc] <==
I0109 07:07:15.139338       1 server_linux.go:53] "Using iptables proxy"
I0109 07:07:15.310539       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I0109 07:07:15.411549       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I0109 07:07:15.411643       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E0109 07:07:15.411843       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0109 07:07:15.443110       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0109 07:07:15.443226       1 server_linux.go:132] "Using iptables Proxier"
I0109 07:07:15.451425       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0109 07:07:15.452573       1 server.go:527] "Version info" version="v1.34.0"
I0109 07:07:15.452601       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0109 07:07:15.456169       1 config.go:200] "Starting service config controller"
I0109 07:07:15.456213       1 config.go:309] "Starting node config controller"
I0109 07:07:15.456228       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I0109 07:07:15.456233       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I0109 07:07:15.456260       1 config.go:106] "Starting endpoint slice config controller"
I0109 07:07:15.456265       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I0109 07:07:15.456409       1 config.go:403] "Starting serviceCIDR config controller"
I0109 07:07:15.456473       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I0109 07:07:15.556703       1 shared_informer.go:356] "Caches are synced" controller="node config"
I0109 07:07:15.556763       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I0109 07:07:15.556789       1 shared_informer.go:356] "Caches are synced" controller="service config"
I0109 07:07:15.556838       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"


==> kube-proxy [d97b4287f69d] <==
I0112 02:40:30.010075       1 server_linux.go:53] "Using iptables proxy"
I0112 02:40:30.221600       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I0112 02:40:30.322458       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I0112 02:40:30.322505       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E0112 02:40:30.322593       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0112 02:40:30.375682       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0112 02:40:30.375753       1 server_linux.go:132] "Using iptables Proxier"
I0112 02:40:30.386115       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0112 02:40:30.392480       1 server.go:527] "Version info" version="v1.34.0"
I0112 02:40:30.392556       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0112 02:40:30.395209       1 config.go:200] "Starting service config controller"
I0112 02:40:30.395251       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I0112 02:40:30.395353       1 config.go:106] "Starting endpoint slice config controller"
I0112 02:40:30.395367       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I0112 02:40:30.395412       1 config.go:309] "Starting node config controller"
I0112 02:40:30.395435       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I0112 02:40:30.395463       1 config.go:403] "Starting serviceCIDR config controller"
I0112 02:40:30.395506       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I0112 02:40:30.498902       1 shared_informer.go:356] "Caches are synced" controller="node config"
I0112 02:40:30.498931       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I0112 02:40:30.498952       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I0112 02:40:30.498959       1 shared_informer.go:356] "Caches are synced" controller="service config"


==> kube-scheduler [0b72ec6fd8ab] <==
I0109 07:07:11.141799       1 serving.go:386] Generated self-signed cert in-memory
I0109 07:07:11.464252       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I0109 07:07:11.464287       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0109 07:07:11.469195       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0109 07:07:11.469252       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I0109 07:07:11.469202       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0109 07:07:11.469600       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0109 07:07:11.469690       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0109 07:07:11.469879       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0109 07:07:11.469882       1 shared_informer.go:349] "Waiting for caches to sync" controller="RequestHeaderAuthRequestController"
I0109 07:07:11.469883       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0109 07:07:11.569964       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0109 07:07:11.570021       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0109 07:07:11.570081       1 shared_informer.go:356] "Caches are synced" controller="RequestHeaderAuthRequestController"
I0109 08:34:37.585412       1 secure_serving.go:259] Stopped listening on 127.0.0.1:10259
I0109 08:34:37.587070       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0109 08:34:37.586839       1 requestheader_controller.go:194] Shutting down RequestHeaderAuthRequestController
I0109 08:34:37.588788       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0109 08:34:37.588770       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0109 08:34:37.589046       1 server.go:263] "[graceful-termination] secure server has stopped listening"
I0109 08:34:37.589145       1 server.go:265] "[graceful-termination] secure server is exiting"
E0109 08:34:37.590242       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [783ed8ba39e2] <==
I0112 02:40:26.275366       1 serving.go:386] Generated self-signed cert in-memory
W0112 02:40:27.585013       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0112 02:40:27.585310       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0112 02:40:27.587562       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0112 02:40:27.587632       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0112 02:40:27.614686       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I0112 02:40:27.614742       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0112 02:40:27.617769       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0112 02:40:27.617867       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0112 02:40:27.618119       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0112 02:40:27.618258       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0112 02:40:27.718396       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Jan 12 02:41:33 minikube kubelet[1432]: I0112 02:41:33.588266    1432 scope.go:117] "RemoveContainer" containerID="1f9a79b519356f9a9ab1afe587c8e0f4d911fd579a7053fd72cf3d269834442f"
Jan 12 02:41:35 minikube kubelet[1432]: I0112 02:41:35.428023    1432 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="01343145-1c5a-44bc-b7cf-72118856e29d" path="/var/lib/kubelet/pods/01343145-1c5a-44bc-b7cf-72118856e29d/volumes"
Jan 12 02:42:10 minikube kubelet[1432]: I0112 02:42:10.914660    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lglk8\" (UniqueName: \"kubernetes.io/projected/e755e878-3645-4d54-b000-18791fa5e434-kube-api-access-lglk8\") pod \"flask-app-deployment-658844656-4qsm9\" (UID: \"e755e878-3645-4d54-b000-18791fa5e434\") " pod="staging/flask-app-deployment-658844656-4qsm9"
Jan 12 02:42:11 minikube kubelet[1432]: I0112 02:42:11.116145    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vlhqq\" (UniqueName: \"kubernetes.io/projected/272b2d64-4a77-4d35-aacb-b9947b2d9d8f-kube-api-access-vlhqq\") pod \"flask-app-deployment-658844656-sgkcv\" (UID: \"272b2d64-4a77-4d35-aacb-b9947b2d9d8f\") " pod="staging/flask-app-deployment-658844656-sgkcv"
Jan 12 02:42:19 minikube kubelet[1432]: I0112 02:42:19.123103    1432 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="staging/flask-app-deployment-658844656-sgkcv" podStartSLOduration=2.605117416 podStartE2EDuration="9.123087415s" podCreationTimestamp="2026-01-12 02:42:10 +0000 UTC" firstStartedPulling="2026-01-12 02:42:11.656685649 +0000 UTC m=+117.492140085" lastFinishedPulling="2026-01-12 02:42:18.174655647 +0000 UTC m=+124.010110084" observedRunningTime="2026-01-12 02:42:19.122734501 +0000 UTC m=+124.958188927" watchObservedRunningTime="2026-01-12 02:42:19.123087415 +0000 UTC m=+124.958541841"
Jan 12 02:42:24 minikube kubelet[1432]: I0112 02:42:24.181873    1432 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="staging/flask-app-deployment-658844656-4qsm9" podStartSLOduration=2.348032905 podStartE2EDuration="14.18185956s" podCreationTimestamp="2026-01-12 02:42:10 +0000 UTC" firstStartedPulling="2026-01-12 02:42:11.656686234 +0000 UTC m=+117.492140660" lastFinishedPulling="2026-01-12 02:42:23.49051289 +0000 UTC m=+129.325967315" observedRunningTime="2026-01-12 02:42:24.181588779 +0000 UTC m=+130.017043205" watchObservedRunningTime="2026-01-12 02:42:24.18185956 +0000 UTC m=+130.017313986"
Jan 12 02:44:22 minikube kubelet[1432]: I0112 02:44:22.850723    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-kjwwg\" (UniqueName: \"kubernetes.io/projected/de8ec402-1be3-4cbd-a4e9-9e02bab00713-kube-api-access-kjwwg\") pod \"obs-stack-kube-prometheus-admission-create-g5hlk\" (UID: \"de8ec402-1be3-4cbd-a4e9-9e02bab00713\") " pod="monitoring/obs-stack-kube-prometheus-admission-create-g5hlk"
Jan 12 02:44:24 minikube kubelet[1432]: I0112 02:44:24.600173    1432 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="41f623cb47dc0b146d602587935ef86ac099cf33a03d4148c585e9e0ccc3b0e9"
Jan 12 02:44:33 minikube kubelet[1432]: I0112 02:44:33.193941    1432 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-kjwwg\" (UniqueName: \"kubernetes.io/projected/de8ec402-1be3-4cbd-a4e9-9e02bab00713-kube-api-access-kjwwg\") pod \"de8ec402-1be3-4cbd-a4e9-9e02bab00713\" (UID: \"de8ec402-1be3-4cbd-a4e9-9e02bab00713\") "
Jan 12 02:44:33 minikube kubelet[1432]: I0112 02:44:33.198891    1432 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/de8ec402-1be3-4cbd-a4e9-9e02bab00713-kube-api-access-kjwwg" (OuterVolumeSpecName: "kube-api-access-kjwwg") pod "de8ec402-1be3-4cbd-a4e9-9e02bab00713" (UID: "de8ec402-1be3-4cbd-a4e9-9e02bab00713"). InnerVolumeSpecName "kube-api-access-kjwwg". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Jan 12 02:44:33 minikube kubelet[1432]: I0112 02:44:33.295168    1432 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-kjwwg\" (UniqueName: \"kubernetes.io/projected/de8ec402-1be3-4cbd-a4e9-9e02bab00713-kube-api-access-kjwwg\") on node \"minikube\" DevicePath \"\""
Jan 12 02:44:33 minikube kubelet[1432]: I0112 02:44:33.702839    1432 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="41f623cb47dc0b146d602587935ef86ac099cf33a03d4148c585e9e0ccc3b0e9"
Jan 12 02:44:34 minikube kubelet[1432]: I0112 02:44:34.534157    1432 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="de8ec402-1be3-4cbd-a4e9-9e02bab00713" path="/var/lib/kubelet/pods/de8ec402-1be3-4cbd-a4e9-9e02bab00713/volumes"
Jan 12 02:44:34 minikube kubelet[1432]: I0112 02:44:34.605127    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"sc-dashboard-provider\" (UniqueName: \"kubernetes.io/configmap/a4982a8f-ec87-4501-a97d-62cc5dc2e58f-sc-dashboard-provider\") pod \"obs-stack-grafana-74958449f-szw2h\" (UID: \"a4982a8f-ec87-4501-a97d-62cc5dc2e58f\") " pod="monitoring/obs-stack-grafana-74958449f-szw2h"
Jan 12 02:44:34 minikube kubelet[1432]: I0112 02:44:34.605246    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"sys\" (UniqueName: \"kubernetes.io/host-path/8694e2df-e2ea-4c78-9f7a-cddd0560cb43-sys\") pod \"obs-stack-prometheus-node-exporter-7nb8d\" (UID: \"8694e2df-e2ea-4c78-9f7a-cddd0560cb43\") " pod="monitoring/obs-stack-prometheus-node-exporter-7nb8d"
Jan 12 02:44:34 minikube kubelet[1432]: I0112 02:44:34.605276    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tx2wv\" (UniqueName: \"kubernetes.io/projected/a4982a8f-ec87-4501-a97d-62cc5dc2e58f-kube-api-access-tx2wv\") pod \"obs-stack-grafana-74958449f-szw2h\" (UID: \"a4982a8f-ec87-4501-a97d-62cc5dc2e58f\") " pod="monitoring/obs-stack-grafana-74958449f-szw2h"
Jan 12 02:44:34 minikube kubelet[1432]: I0112 02:44:34.605296    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"proc\" (UniqueName: \"kubernetes.io/host-path/8694e2df-e2ea-4c78-9f7a-cddd0560cb43-proc\") pod \"obs-stack-prometheus-node-exporter-7nb8d\" (UID: \"8694e2df-e2ea-4c78-9f7a-cddd0560cb43\") " pod="monitoring/obs-stack-prometheus-node-exporter-7nb8d"
Jan 12 02:44:34 minikube kubelet[1432]: I0112 02:44:34.605315    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"storage\" (UniqueName: \"kubernetes.io/empty-dir/a4982a8f-ec87-4501-a97d-62cc5dc2e58f-storage\") pod \"obs-stack-grafana-74958449f-szw2h\" (UID: \"a4982a8f-ec87-4501-a97d-62cc5dc2e58f\") " pod="monitoring/obs-stack-grafana-74958449f-szw2h"
Jan 12 02:44:34 minikube kubelet[1432]: I0112 02:44:34.605335    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"search\" (UniqueName: \"kubernetes.io/empty-dir/a4982a8f-ec87-4501-a97d-62cc5dc2e58f-search\") pod \"obs-stack-grafana-74958449f-szw2h\" (UID: \"a4982a8f-ec87-4501-a97d-62cc5dc2e58f\") " pod="monitoring/obs-stack-grafana-74958449f-szw2h"
Jan 12 02:44:34 minikube kubelet[1432]: I0112 02:44:34.605353    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"sc-datasources-volume\" (UniqueName: \"kubernetes.io/empty-dir/a4982a8f-ec87-4501-a97d-62cc5dc2e58f-sc-datasources-volume\") pod \"obs-stack-grafana-74958449f-szw2h\" (UID: \"a4982a8f-ec87-4501-a97d-62cc5dc2e58f\") " pod="monitoring/obs-stack-grafana-74958449f-szw2h"
Jan 12 02:44:34 minikube kubelet[1432]: I0112 02:44:34.605373    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vsmvt\" (UniqueName: \"kubernetes.io/projected/e0ac4bcd-a484-4f05-91b0-b5edd79d208b-kube-api-access-vsmvt\") pod \"obs-stack-kube-prometheus-operator-64587f75b5-8xwm8\" (UID: \"e0ac4bcd-a484-4f05-91b0-b5edd79d208b\") " pod="monitoring/obs-stack-kube-prometheus-operator-64587f75b5-8xwm8"
Jan 12 02:44:34 minikube kubelet[1432]: I0112 02:44:34.605413    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-prvhk\" (UniqueName: \"kubernetes.io/projected/99763dcc-a713-4f00-9f1b-b5066632eff1-kube-api-access-prvhk\") pod \"obs-stack-kube-state-metrics-7c989b94c7-jgzdp\" (UID: \"99763dcc-a713-4f00-9f1b-b5066632eff1\") " pod="monitoring/obs-stack-kube-state-metrics-7c989b94c7-jgzdp"
Jan 12 02:44:34 minikube kubelet[1432]: I0112 02:44:34.605460    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config\" (UniqueName: \"kubernetes.io/configmap/a4982a8f-ec87-4501-a97d-62cc5dc2e58f-config\") pod \"obs-stack-grafana-74958449f-szw2h\" (UID: \"a4982a8f-ec87-4501-a97d-62cc5dc2e58f\") " pod="monitoring/obs-stack-grafana-74958449f-szw2h"
Jan 12 02:44:34 minikube kubelet[1432]: I0112 02:44:34.605518    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tls-secret\" (UniqueName: \"kubernetes.io/secret/e0ac4bcd-a484-4f05-91b0-b5edd79d208b-tls-secret\") pod \"obs-stack-kube-prometheus-operator-64587f75b5-8xwm8\" (UID: \"e0ac4bcd-a484-4f05-91b0-b5edd79d208b\") " pod="monitoring/obs-stack-kube-prometheus-operator-64587f75b5-8xwm8"
Jan 12 02:44:34 minikube kubelet[1432]: I0112 02:44:34.605594    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"sc-dashboard-volume\" (UniqueName: \"kubernetes.io/empty-dir/a4982a8f-ec87-4501-a97d-62cc5dc2e58f-sc-dashboard-volume\") pod \"obs-stack-grafana-74958449f-szw2h\" (UID: \"a4982a8f-ec87-4501-a97d-62cc5dc2e58f\") " pod="monitoring/obs-stack-grafana-74958449f-szw2h"
Jan 12 02:44:34 minikube kubelet[1432]: I0112 02:44:34.605625    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"root\" (UniqueName: \"kubernetes.io/host-path/8694e2df-e2ea-4c78-9f7a-cddd0560cb43-root\") pod \"obs-stack-prometheus-node-exporter-7nb8d\" (UID: \"8694e2df-e2ea-4c78-9f7a-cddd0560cb43\") " pod="monitoring/obs-stack-prometheus-node-exporter-7nb8d"
Jan 12 02:44:36 minikube kubelet[1432]: I0112 02:44:36.727557    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qz2bv\" (UniqueName: \"kubernetes.io/projected/46290789-0243-4e62-a1ed-4161a42a786a-kube-api-access-qz2bv\") pod \"obs-stack-kube-prometheus-admission-patch-9q8k2\" (UID: \"46290789-0243-4e62-a1ed-4161a42a786a\") " pod="monitoring/obs-stack-kube-prometheus-admission-patch-9q8k2"
Jan 12 02:44:39 minikube kubelet[1432]: I0112 02:44:39.252753    1432 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-qz2bv\" (UniqueName: \"kubernetes.io/projected/46290789-0243-4e62-a1ed-4161a42a786a-kube-api-access-qz2bv\") pod \"46290789-0243-4e62-a1ed-4161a42a786a\" (UID: \"46290789-0243-4e62-a1ed-4161a42a786a\") "
Jan 12 02:44:39 minikube kubelet[1432]: I0112 02:44:39.256308    1432 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/46290789-0243-4e62-a1ed-4161a42a786a-kube-api-access-qz2bv" (OuterVolumeSpecName: "kube-api-access-qz2bv") pod "46290789-0243-4e62-a1ed-4161a42a786a" (UID: "46290789-0243-4e62-a1ed-4161a42a786a"). InnerVolumeSpecName "kube-api-access-qz2bv". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Jan 12 02:44:39 minikube kubelet[1432]: I0112 02:44:39.353300    1432 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-qz2bv\" (UniqueName: \"kubernetes.io/projected/46290789-0243-4e62-a1ed-4161a42a786a-kube-api-access-qz2bv\") on node \"minikube\" DevicePath \"\""
Jan 12 02:44:37 minikube kubelet[1432]: I0112 02:44:37.377408    1432 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a88dd80ab991edef438df438f14f58aa20f87ba93cda72fb21113469c3bbf93b"
Jan 12 02:44:38 minikube kubelet[1432]: I0112 02:44:38.034964    1432 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="46290789-0243-4e62-a1ed-4161a42a786a" path="/var/lib/kubelet/pods/46290789-0243-4e62-a1ed-4161a42a786a/volumes"
Jan 12 02:44:47 minikube kubelet[1432]: I0112 02:44:47.491138    1432 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="monitoring/obs-stack-prometheus-node-exporter-7nb8d" podStartSLOduration=-1.066533824 podStartE2EDuration="13.491095802s" podCreationTimestamp="2026-01-12 02:44:34 +0000 UTC" firstStartedPulling="2026-01-12 02:44:35.040168559 +0000 UTC m=+270.806653506" lastFinishedPulling="2026-01-12 02:44:47.098473875 +0000 UTC m=+285.364283132" observedRunningTime="2026-01-12 02:44:47.490963304 +0000 UTC m=+285.756772582" watchObservedRunningTime="2026-01-12 02:44:47.491095802 +0000 UTC m=+285.756905059"
Jan 12 02:44:56 minikube kubelet[1432]: I0112 02:44:56.586570    1432 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="monitoring/obs-stack-kube-state-metrics-7c989b94c7-jgzdp" podStartSLOduration=-0.875401876 podStartE2EDuration="22.586551367s" podCreationTimestamp="2026-01-12 02:44:34 +0000 UTC" firstStartedPulling="2026-01-12 02:44:35.158524985 +0000 UTC m=+270.925009932" lastFinishedPulling="2026-01-12 02:44:56.121153918 +0000 UTC m=+294.386963175" observedRunningTime="2026-01-12 02:44:56.586200798 +0000 UTC m=+294.852010087" watchObservedRunningTime="2026-01-12 02:44:56.586551367 +0000 UTC m=+294.852360635"
Jan 12 02:45:03 minikube kubelet[1432]: I0112 02:45:03.181571    1432 scope.go:117] "RemoveContainer" containerID="68ac13d2caa3987652d42e437fd29265cd00766638021c8da869577f2253445d"
Jan 12 02:45:03 minikube kubelet[1432]: I0112 02:45:03.196745    1432 scope.go:117] "RemoveContainer" containerID="7ac551edee0f38eea0f40dc0dc84620c12e47dca0ab310dd65aae521805394f1"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.285502    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/secret/6f6bfd1b-3659-4431-8012-14b09ccbf896-config-volume\") pod \"alertmanager-obs-stack-kube-prometheus-alertmanager-0\" (UID: \"6f6bfd1b-3659-4431-8012-14b09ccbf896\") " pod="monitoring/alertmanager-obs-stack-kube-prometheus-alertmanager-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.285566    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-out\" (UniqueName: \"kubernetes.io/empty-dir/6f6bfd1b-3659-4431-8012-14b09ccbf896-config-out\") pod \"alertmanager-obs-stack-kube-prometheus-alertmanager-0\" (UID: \"6f6bfd1b-3659-4431-8012-14b09ccbf896\") " pod="monitoring/alertmanager-obs-stack-kube-prometheus-alertmanager-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.285588    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"alertmanager-obs-stack-kube-prometheus-alertmanager-db\" (UniqueName: \"kubernetes.io/empty-dir/6f6bfd1b-3659-4431-8012-14b09ccbf896-alertmanager-obs-stack-kube-prometheus-alertmanager-db\") pod \"alertmanager-obs-stack-kube-prometheus-alertmanager-0\" (UID: \"6f6bfd1b-3659-4431-8012-14b09ccbf896\") " pod="monitoring/alertmanager-obs-stack-kube-prometheus-alertmanager-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.285609    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"web-config\" (UniqueName: \"kubernetes.io/secret/6f6bfd1b-3659-4431-8012-14b09ccbf896-web-config\") pod \"alertmanager-obs-stack-kube-prometheus-alertmanager-0\" (UID: \"6f6bfd1b-3659-4431-8012-14b09ccbf896\") " pod="monitoring/alertmanager-obs-stack-kube-prometheus-alertmanager-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.285627    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cluster-tls-config\" (UniqueName: \"kubernetes.io/secret/6f6bfd1b-3659-4431-8012-14b09ccbf896-cluster-tls-config\") pod \"alertmanager-obs-stack-kube-prometheus-alertmanager-0\" (UID: \"6f6bfd1b-3659-4431-8012-14b09ccbf896\") " pod="monitoring/alertmanager-obs-stack-kube-prometheus-alertmanager-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.285648    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vdbr5\" (UniqueName: \"kubernetes.io/projected/6f6bfd1b-3659-4431-8012-14b09ccbf896-kube-api-access-vdbr5\") pod \"alertmanager-obs-stack-kube-prometheus-alertmanager-0\" (UID: \"6f6bfd1b-3659-4431-8012-14b09ccbf896\") " pod="monitoring/alertmanager-obs-stack-kube-prometheus-alertmanager-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.285681    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tls-assets\" (UniqueName: \"kubernetes.io/projected/6f6bfd1b-3659-4431-8012-14b09ccbf896-tls-assets\") pod \"alertmanager-obs-stack-kube-prometheus-alertmanager-0\" (UID: \"6f6bfd1b-3659-4431-8012-14b09ccbf896\") " pod="monitoring/alertmanager-obs-stack-kube-prometheus-alertmanager-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.385961    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-out\" (UniqueName: \"kubernetes.io/empty-dir/88e67b63-36ed-48de-95cd-2242b79ddd0f-config-out\") pod \"prometheus-obs-stack-kube-prometheus-prometheus-0\" (UID: \"88e67b63-36ed-48de-95cd-2242b79ddd0f\") " pod="monitoring/prometheus-obs-stack-kube-prometheus-prometheus-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.386095    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"prometheus-obs-stack-kube-prometheus-prometheus-rulefiles-2\" (UniqueName: \"kubernetes.io/configmap/88e67b63-36ed-48de-95cd-2242b79ddd0f-prometheus-obs-stack-kube-prometheus-prometheus-rulefiles-2\") pod \"prometheus-obs-stack-kube-prometheus-prometheus-0\" (UID: \"88e67b63-36ed-48de-95cd-2242b79ddd0f\") " pod="monitoring/prometheus-obs-stack-kube-prometheus-prometheus-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.386149    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"prometheus-obs-stack-kube-prometheus-prometheus-rulefiles-0\" (UniqueName: \"kubernetes.io/configmap/88e67b63-36ed-48de-95cd-2242b79ddd0f-prometheus-obs-stack-kube-prometheus-prometheus-rulefiles-0\") pod \"prometheus-obs-stack-kube-prometheus-prometheus-0\" (UID: \"88e67b63-36ed-48de-95cd-2242b79ddd0f\") " pod="monitoring/prometheus-obs-stack-kube-prometheus-prometheus-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.386241    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hpvd9\" (UniqueName: \"kubernetes.io/projected/88e67b63-36ed-48de-95cd-2242b79ddd0f-kube-api-access-hpvd9\") pod \"prometheus-obs-stack-kube-prometheus-prometheus-0\" (UID: \"88e67b63-36ed-48de-95cd-2242b79ddd0f\") " pod="monitoring/prometheus-obs-stack-kube-prometheus-prometheus-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.386354    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"prometheus-obs-stack-kube-prometheus-prometheus-rulefiles-1\" (UniqueName: \"kubernetes.io/configmap/88e67b63-36ed-48de-95cd-2242b79ddd0f-prometheus-obs-stack-kube-prometheus-prometheus-rulefiles-1\") pod \"prometheus-obs-stack-kube-prometheus-prometheus-0\" (UID: \"88e67b63-36ed-48de-95cd-2242b79ddd0f\") " pod="monitoring/prometheus-obs-stack-kube-prometheus-prometheus-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.386438    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"web-config\" (UniqueName: \"kubernetes.io/secret/88e67b63-36ed-48de-95cd-2242b79ddd0f-web-config\") pod \"prometheus-obs-stack-kube-prometheus-prometheus-0\" (UID: \"88e67b63-36ed-48de-95cd-2242b79ddd0f\") " pod="monitoring/prometheus-obs-stack-kube-prometheus-prometheus-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.386473    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config\" (UniqueName: \"kubernetes.io/secret/88e67b63-36ed-48de-95cd-2242b79ddd0f-config\") pod \"prometheus-obs-stack-kube-prometheus-prometheus-0\" (UID: \"88e67b63-36ed-48de-95cd-2242b79ddd0f\") " pod="monitoring/prometheus-obs-stack-kube-prometheus-prometheus-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.386510    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"prometheus-obs-stack-kube-prometheus-prometheus-db\" (UniqueName: \"kubernetes.io/empty-dir/88e67b63-36ed-48de-95cd-2242b79ddd0f-prometheus-obs-stack-kube-prometheus-prometheus-db\") pod \"prometheus-obs-stack-kube-prometheus-prometheus-0\" (UID: \"88e67b63-36ed-48de-95cd-2242b79ddd0f\") " pod="monitoring/prometheus-obs-stack-kube-prometheus-prometheus-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.386953    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tls-assets\" (UniqueName: \"kubernetes.io/projected/88e67b63-36ed-48de-95cd-2242b79ddd0f-tls-assets\") pod \"prometheus-obs-stack-kube-prometheus-prometheus-0\" (UID: \"88e67b63-36ed-48de-95cd-2242b79ddd0f\") " pod="monitoring/prometheus-obs-stack-kube-prometheus-prometheus-0"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.721773    1432 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="monitoring/obs-stack-kube-prometheus-operator-64587f75b5-8xwm8" podStartSLOduration=-0.302475808 podStartE2EDuration="32.721751995s" podCreationTimestamp="2026-01-12 02:44:34 +0000 UTC" firstStartedPulling="2026-01-12 02:44:35.165603793 +0000 UTC m=+270.932088739" lastFinishedPulling="2026-01-12 02:45:05.690507285 +0000 UTC m=+303.956316542" observedRunningTime="2026-01-12 02:45:06.721218831 +0000 UTC m=+304.987028098" watchObservedRunningTime="2026-01-12 02:45:06.721751995 +0000 UTC m=+304.987561263"
Jan 12 02:45:06 minikube kubelet[1432]: I0112 02:45:06.848968    1432 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="789351a0c571b87d756f67764fc75f8a31d82df27303d30c5c14bbc9635f8c8d"
Jan 12 02:46:00 minikube kubelet[1432]: I0112 02:46:00.643729    1432 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="monitoring/obs-stack-grafana-74958449f-szw2h" podStartSLOduration=-5.229820147 podStartE2EDuration="1m26.643709047s" podCreationTimestamp="2026-01-12 02:44:34 +0000 UTC" firstStartedPulling="2026-01-12 02:44:35.177662119 +0000 UTC m=+270.944147055" lastFinishedPulling="2026-01-12 02:45:59.581775173 +0000 UTC m=+362.817676249" observedRunningTime="2026-01-12 02:46:00.643271502 +0000 UTC m=+363.879172578" watchObservedRunningTime="2026-01-12 02:46:00.643709047 +0000 UTC m=+363.879610123"
Jan 12 02:46:19 minikube kubelet[1432]: I0112 02:46:19.441318    1432 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="monitoring/alertmanager-obs-stack-kube-prometheus-alertmanager-0" podStartSLOduration=-5.630319128 podStartE2EDuration="1m13.441301412s" podCreationTimestamp="2026-01-12 02:45:06 +0000 UTC" firstStartedPulling="2026-01-12 02:45:06.924177179 +0000 UTC m=+305.189986446" lastFinishedPulling="2026-01-12 02:46:18.572908499 +0000 UTC m=+384.261606986" observedRunningTime="2026-01-12 02:46:19.440910183 +0000 UTC m=+385.129608681" watchObservedRunningTime="2026-01-12 02:46:19.441301412 +0000 UTC m=+385.129999910"
Jan 12 02:46:36 minikube kubelet[1432]: I0112 02:46:36.717273    1432 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="monitoring/prometheus-obs-stack-kube-prometheus-prometheus-0" podStartSLOduration=-4.7926957869999995 podStartE2EDuration="1m30.717253512s" podCreationTimestamp="2026-01-12 02:45:06 +0000 UTC" firstStartedPulling="2026-01-12 02:45:07.065357738 +0000 UTC m=+305.331166995" lastFinishedPulling="2026-01-12 02:46:35.152417806 +0000 UTC m=+400.841116294" observedRunningTime="2026-01-12 02:46:36.716802813 +0000 UTC m=+402.405501333" watchObservedRunningTime="2026-01-12 02:46:36.717253512 +0000 UTC m=+402.405952000"
Jan 12 03:00:49 minikube kubelet[1432]: I0112 03:00:49.989840    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vbtbd\" (UniqueName: \"kubernetes.io/projected/b617fce0-e203-47c5-bf6c-4a2ebd3db974-kube-api-access-vbtbd\") pod \"metrics-server-85b7d694d7-7pqsn\" (UID: \"b617fce0-e203-47c5-bf6c-4a2ebd3db974\") " pod="kube-system/metrics-server-85b7d694d7-7pqsn"
Jan 12 03:00:49 minikube kubelet[1432]: I0112 03:00:49.990169    1432 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-dir\" (UniqueName: \"kubernetes.io/empty-dir/b617fce0-e203-47c5-bf6c-4a2ebd3db974-tmp-dir\") pod \"metrics-server-85b7d694d7-7pqsn\" (UID: \"b617fce0-e203-47c5-bf6c-4a2ebd3db974\") " pod="kube-system/metrics-server-85b7d694d7-7pqsn"
Jan 12 03:01:01 minikube kubelet[1432]: I0112 03:01:01.275164    1432 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/metrics-server-85b7d694d7-7pqsn" podStartSLOduration=2.770139813 podStartE2EDuration="12.27496131s" podCreationTimestamp="2026-01-12 03:00:49 +0000 UTC" firstStartedPulling="2026-01-12 03:00:50.758825322 +0000 UTC m=+1328.630398951" lastFinishedPulling="2026-01-12 03:01:00.263646819 +0000 UTC m=+1338.135220448" observedRunningTime="2026-01-12 03:01:01.274783084 +0000 UTC m=+1339.146356788" watchObservedRunningTime="2026-01-12 03:01:01.27496131 +0000 UTC m=+1339.146534938"


==> storage-provisioner [c7153b7e079a] <==
W0112 03:08:36.958093       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:36.964162       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:38.969363       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:38.974804       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:40.978367       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:40.984304       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:42.987166       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:42.995995       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:45.000553       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:45.006267       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:44.536712       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:44.539376       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:46.543235       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:46.549411       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:48.551921       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:48.558137       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:50.563410       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:50.568909       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:52.573305       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:52.581116       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:54.585166       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:54.590797       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:56.595147       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:56.603788       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:58.608520       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:08:58.618194       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:00.620815       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:00.626937       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:02.629884       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:02.642168       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:04.644902       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:04.650963       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:06.656762       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:06.663314       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:08.666439       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:08.672387       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:10.676367       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:10.685287       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:12.688400       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:12.695631       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:14.699369       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:14.705587       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:14.199677       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:14.202180       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:16.209942       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:16.215947       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:18.219006       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:18.224699       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:20.228122       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:20.234761       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:22.237783       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:22.244329       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:24.247956       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:24.254599       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:26.257729       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:26.264322       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:28.267942       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:28.274501       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:30.278466       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0112 03:09:30.288651       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [eee775c0d4ff] <==
I0112 02:40:29.764920       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0112 02:40:57.298721       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

